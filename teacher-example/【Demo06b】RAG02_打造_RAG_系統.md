# ã€Demo06bã€‘RAG02_æ‰“é€ _RAG_ç³»çµ±

*This notebook was created for Google Colab*

*Language: python*

---


### 0. è®€å…¥ä½ æ‰“é€ å¥½çš„ vector dataset



```python
!pip -q install gdown
```



```python
GDRIVE_PUBLIC_URL = "https://drive.google.com/file/d/1raCLp-CkB4scczsHU-BWwG1Tv4-9pWOU/view?usp=sharing"
```



```python
!gdown --fuzzy -O faiss_db.zip "{GDRIVE_PUBLIC_URL}"
```



```python
!unzip faiss_db.zip
```


### 1. å®‰è£ä¸¦å¼•å…¥å¿…è¦å¥—ä»¶



```python
!pip install -U langchain langchain-community faiss-cpu transformers sentence-transformers huggingface_hub
!pip -q install "aisuite[all]"
```



```python
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
```



```python
from openai import OpenAI
import gradio as gr
```


### 2. è‡ªè¨‚ E5 embedding é¡åˆ¥



```python
import os
from google.colab import userdata
```



```python
hf_token = userdata.get('HuggingFace')
```



```python
from huggingface_hub import login
login(token=hf_token)
```



```python
class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings):
    def __init__(self, **kwargs):
        super().__init__(
            model_name="google/embeddinggemma-300m",
            encode_kwargs={"normalize_embeddings": True},
            **kwargs
        )

    def embed_documents(self, texts):
        # ä½ ä¹Ÿå¯ä»¥æŠŠ "none" æ”¹æˆçœŸå¯¦æ¨™é¡Œï¼ˆæª”å/ç« ç¯€åï¼‰ï¼Œæ•ˆæœæœƒæ›´ç©©
        texts = [f"title: none | text: {t}" for t in texts]
        return super().embed_documents(texts)

    def embed_query(self, text):
        # å®˜æ–¹æª¢ç´¢å»ºè­°å‰ç¶´
        return super().embed_query(f"task: search result | query: {text}")
```


### 3. è¼‰å…¥ `faiss_db`



```python
embedding_model = EmbeddingGemmaEmbeddings()
vectorstore = FAISS.load_local(
    "faiss_db",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)
```



```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
```


### 4. è¨­å®šå¥½æˆ‘å€‘è¦çš„ LLM


å¦‚ä¹‹å‰, æˆ‘å€‘æœƒç”¨ OpenAI APIã€‚é€™è£¡ä½¿ç”¨ Groq æœå‹™, å¯æ”¹æˆä½ è¦çš„æœå‹™ã€‚



```python
import aisuite as ai
```



```python
api_key = userdata.get('Groq')
```



```python
os.environ['GROQ_API_KEY']=api_key
```


é€™è£¡çš„æ¨¡å‹å’Œ `base_url` æ˜¯ç”¨ Groq, å¦‚æœç”¨å…¶ä»–æœå‹™è«‹è‡ªè¡Œä¿®æ”¹ã€‚



```python
model = "groq:openai/gpt-oss-120b"
#base_url="https://api.groq.com/openai/v1"
```



```python
client = ai.Client()
```


### 5. `prompt` è¨­è¨ˆ



```python
system_prompt = "ä½ æ˜¯æ”¿å¤§çš„ AI è‡ªä¸»å­¸ç¿’è¼”å°å“¡ï¼Œè«‹æ ¹æ“šè³‡æ–™ä¾†å›æ‡‰å­¸ç”Ÿçš„å•é¡Œã€‚è«‹è¦ªåˆ‡ã€ç°¡æ½”ä¸¦é™„å¸¶å…·é«”å»ºè­°ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›æ‡‰ã€‚"

prompt_template = """
æ ¹æ“šä¸‹åˆ—è³‡æ–™ï¼š
{retrieved_chunks}

å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼š{question}

è«‹æ ¹æ“šè³‡æ–™å…§å®¹å›è¦†ï¼Œè‹¥è³‡æ–™ä¸è¶³è«‹å‘Šè¨´åŒå­¸å¯ä»¥è«‹æ•™å­¸å‹™è™•ç”Ÿåƒ‘çµ„çš„è€å¸«ã€‚
"""
```


### 6. ä½¿ç”¨ RAG ä¾†å›æ‡‰

æœå°‹èˆ‡ä½¿ç”¨è€…å•é¡Œç›¸é—œçš„è³‡è¨Šï¼Œæ ¹æ“šæˆ‘å€‘çš„ prompt æ¨£ç‰ˆå»è®“ LLM å›æ‡‰ã€‚



```python
chat_history = []

def chat_with_rag(user_input):
    global chat_history
    # å–å›ç›¸é—œè³‡æ–™
    docs = retriever.get_relevant_documents(user_input)
    retrieved_chunks = "\n\n".join([doc.page_content for doc in docs])

    # å°‡è‡ªå®š prompt å¥—å…¥æ ¼å¼
    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)

    # ç”¨ AI Suite å‘¼å«èªè¨€æ¨¡å‹
    response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": final_prompt},
    ]
    )
    answer = response.choices[0].message.content

    chat_history.append((user_input, answer))
    return answer
```


### 7. ç”¨ Gradio æ‰“é€  Web App



```python
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ“ AI å­¸æ ¡çæ‡²è«®è©¢å¸«")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="è«‹è¼¸å…¥ä½ çš„å•é¡Œ...")

    def respond(message, chat_history_local):
        response = chat_with_rag(message)
        chat_history_local.append((message, response))
        return "", chat_history_local

    msg.submit(respond, [msg, chatbot], [msg, chatbot])

demo.launch(debug=True)
```

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "GPT-5 API 程式設計助手 - Part B: RAG 對話機器人\n",
        "\n",
        "本地執行:\n",
        "    cd Week7\n",
        "    uv run python rag_chatbot.py\n",
        "\n",
        "Google Colab 執行:\n",
        "    # 1. 安裝套件\n",
        "    !pip install openai faiss-cpu numpy gradio\n",
        "\n",
        "    # 2. 下載向量資料庫\n",
        "    GDRIVE_LINK = \"YOUR_GDRIVE_LINK_HERE\"\n",
        "    !gdown --fuzzy {GDRIVE_LINK}\n",
        "    !unzip -o faiss_db.zip\n",
        "\n",
        "    # 3. 設定 API key\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    # 4. 執行程式\n",
        "    !python rag_chatbot.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# ===== 全域設定 =====\n",
        "GDRIVE_FILE_ID = \"YOUR_FILE_ID_HERE\"  # 待填入\n",
        "FAISS_DB_PATH = \"faiss_db\"\n",
        "MAX_TOOL_ITERATIONS = 5\n",
        "\n",
        "# System prompt for the chatbot\n",
        "SYSTEM_PROMPT = \"\"\"你是一位專業的 GPT-5 Response API 程式設計助手。\n",
        "\n",
        "你的專長領域:\n",
        "- GPT-5 Response API 的使用方式\n",
        "- Function calling 和 custom tools\n",
        "- Reasoning effort 控制 (minimal, low, medium, high)\n",
        "- Text verbosity 設定\n",
        "- Tool calling 的最佳實踐\n",
        "\n",
        "回答風格:\n",
        "- 友善、專業、有耐心\n",
        "- 提供清楚的程式碼範例\n",
        "- 解釋技術概念時用簡單易懂的方式\n",
        "- 如果不確定答案,誠實告知並建議查閱官方文件\n",
        "- 所有程式碼範例或任何與程式碼相關的內容都必須放在 code block 中,否則不易閱讀\n",
        "\n",
        "重要: 當你需要查詢 GPT-5 API 相關資料時,請使用 search_chunks 工具搜尋文件內容。\"\"\"\n",
        "\n",
        "\n",
        "# ===== 1. 環境偵測與初始化 =====\n",
        "\n",
        "def is_colab() -> bool:\n",
        "    \"\"\"偵測是否在 Google Colab 環境\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def setup_environment() -> str:\n",
        "    \"\"\"\n",
        "    設定環境並取得 API key\n",
        "\n",
        "    Returns:\n",
        "        str: OpenAI API key\n",
        "    \"\"\"\n",
        "    if is_colab():\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "        print(\"✓ Colab 環境: 從 userdata 載入 API key\")\n",
        "    else:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "        print(\"✓ 本地環境: 從環境變數載入 API key\")\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"未找到 OPENAI_API_KEY,請確認環境設定\")\n",
        "\n",
        "    return api_key\n",
        "\n",
        "\n",
        "def download_and_extract_faiss(gdrive_url: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    在 Colab 環境下載並解壓縮 FAISS 資料庫\n",
        "    本地環境跳過此步驟\n",
        "\n",
        "    Args:\n",
        "        gdrive_url: Google Drive 分享連結 (可選)\n",
        "    \"\"\"\n",
        "    if not is_colab():\n",
        "        print(\"✓ 本地環境: 跳過下載,直接使用本地 faiss_db\")\n",
        "        return\n",
        "\n",
        "    if not gdrive_url or gdrive_url == \"YOUR_GDRIVE_LINK_HERE\":\n",
        "        print(\"⚠️  警告: 尚未設定 Google Drive 連結\")\n",
        "        print(\"請在程式碼中設定 GDRIVE_FILE_ID 或傳入 gdrive_url 參數\")\n",
        "        return\n",
        "\n",
        "    print(\"⬇️  Colab 環境: 下載 faiss_db.zip...\")\n",
        "    os.system(f'gdown --fuzzy \"{gdrive_url}\"')\n",
        "\n",
        "    print(\"📦 解壓縮 faiss_db.zip...\")\n",
        "    os.system('unzip -o faiss_db.zip')\n",
        "\n",
        "    print(\"✓ FAISS 資料庫準備完成\")\n",
        "\n",
        "\n",
        "# ===== 2. FAISS 載入 =====\n",
        "\n",
        "def load_vectorstore() -> Tuple[faiss.Index, List[Dict], OpenAI]:\n",
        "    \"\"\"\n",
        "    載入 FAISS 向量資料庫\n",
        "\n",
        "    Returns:\n",
        "        Tuple[faiss.Index, List[Dict], OpenAI]: (FAISS index, metadata, OpenAI client)\n",
        "    \"\"\"\n",
        "    print(f\"📂 載入向量資料庫: {FAISS_DB_PATH}/\")\n",
        "\n",
        "    # 載入 FAISS index\n",
        "    index_path = os.path.join(FAISS_DB_PATH, \"index.faiss\")\n",
        "    if not os.path.exists(index_path):\n",
        "        raise FileNotFoundError(f\"找不到 FAISS index: {index_path}\")\n",
        "\n",
        "    index = faiss.read_index(index_path)\n",
        "    print(f\"  ✓ FAISS index 載入完成: {index.ntotal} 個向量\")\n",
        "\n",
        "    # 載入 metadata\n",
        "    metadata_path = os.path.join(FAISS_DB_PATH, \"metadata.json\")\n",
        "    if not os.path.exists(metadata_path):\n",
        "        raise FileNotFoundError(f\"找不到 metadata: {metadata_path}\")\n",
        "\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"  ✓ Metadata 載入完成: {len(metadata)} 個 chunks\")\n",
        "\n",
        "    # 初始化 OpenAI client\n",
        "    client = OpenAI()\n",
        "\n",
        "    return index, metadata, client\n",
        "\n",
        "\n",
        "# ===== 3. RAG 檢索函數 =====\n",
        "\n",
        "def search_chunks(query: str, index: faiss.Index, metadata: List[Dict],\n",
        "                 client: OpenAI, k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    使用 FAISS 搜尋與問題最相關的 chunks\n",
        "\n",
        "    Args:\n",
        "        query: 使用者問題\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "        k: 回傳結果數量\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: 相關的 chunks\n",
        "    \"\"\"\n",
        "    # 將 query 轉成 embedding\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-large\",\n",
        "        input=[query]\n",
        "    )\n",
        "    query_embedding = response.data[0].embedding\n",
        "\n",
        "    # FAISS 搜尋\n",
        "    query_vector = np.array([query_embedding]).astype('float32')\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "\n",
        "    # 取得相關 chunks\n",
        "    results = []\n",
        "    for idx, distance in zip(indices[0], distances[0]):\n",
        "        if idx < len(metadata):\n",
        "            chunk = metadata[idx].copy()\n",
        "            chunk['distance'] = float(distance)\n",
        "            results.append(chunk)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def format_chunks_for_llm(chunks: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    格式化 chunks 成適合 LLM 閱讀的格式\n",
        "\n",
        "    Args:\n",
        "        chunks: 搜尋結果 chunks\n",
        "\n",
        "    Returns:\n",
        "        str: 格式化後的文字\n",
        "    \"\"\"\n",
        "    formatted_parts = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        source = chunk.get('source_file', 'Unknown')\n",
        "        text = chunk.get('text', '')\n",
        "\n",
        "        formatted_parts.append(f\"[文件 {i}: {source}]\\n{text}\")\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(formatted_parts)\n",
        "\n",
        "\n",
        "# ===== 4. GPT-5 對話引擎 =====\n",
        "\n",
        "def convert_gradio_history_to_openai(history: List[Tuple[str, str]]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    轉換 Gradio history 格式為 OpenAI messages 格式\n",
        "\n",
        "    Args:\n",
        "        history: Gradio history [(user_msg, bot_msg), ...]\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: OpenAI messages 格式\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "\n",
        "    for user_msg, bot_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "def chat_with_rag(user_message: str, gradio_history: List[Tuple[str, str]],\n",
        "                 index: faiss.Index, metadata: List[Dict],\n",
        "                 client: OpenAI) -> str:\n",
        "    \"\"\"\n",
        "    主要對話函數,整合 RAG 檢索和 GPT-5 回答\n",
        "\n",
        "    Args:\n",
        "        user_message: 使用者輸入\n",
        "        gradio_history: Gradio 對話歷史\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "\n",
        "    Returns:\n",
        "        str: AI 回答\n",
        "    \"\"\"\n",
        "    # 1. 轉換對話歷史\n",
        "    input_messages = convert_gradio_history_to_openai(gradio_history)\n",
        "    input_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # 2. 定義 search_chunks tool\n",
        "    tools = [{\n",
        "        \"type\": \"function\",\n",
        "        \"name\": \"search_chunks\",\n",
        "        \"description\": \"搜尋 GPT-5 Response API 官方文件內容。當使用者詢問關於 GPT-5 API、function calling、tool calling、reasoning effort、verbosity 等技術問題時,請使用此工具搜尋相關文件。\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"搜尋關鍵字或問題,例如: 'custom tools', 'reasoning effort', 'function calling example'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "            \"additionalProperties\": False\n",
        "        },\n",
        "        \"strict\": True\n",
        "    }]\n",
        "\n",
        "    # 3. Tool calling loop\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < MAX_TOOL_ITERATIONS:\n",
        "        print(f\"\\n🔄 Tool calling iteration {iteration + 1}\")\n",
        "\n",
        "        # 呼叫 GPT-5\n",
        "        response = client.responses.create(\n",
        "            model=\"gpt-5\",\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "            instructions=SYSTEM_PROMPT,\n",
        "            input=input_messages,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        # 將 response.output 的每個 item 加到 input_messages\n",
        "        # 重要: 不要直接 += response.output，要逐個 append\n",
        "        for item in response.output:\n",
        "            input_messages.append(item)\n",
        "\n",
        "        # 檢查是否有 function calls 並執行\n",
        "        has_tool_calls = False\n",
        "\n",
        "        for item in response.output:\n",
        "            if item.type == \"function_call\":\n",
        "                has_tool_calls = True\n",
        "                function_name = item.name\n",
        "                call_id = item.call_id\n",
        "\n",
        "                print(f\"  📞 GPT-5 呼叫工具: {function_name}\")\n",
        "                print(f\"     參數: {item.arguments}\")\n",
        "\n",
        "                if function_name == \"search_chunks\":\n",
        "                    # 解析參數\n",
        "                    args = json.loads(item.arguments)\n",
        "                    query = args.get(\"query\", \"\")\n",
        "\n",
        "                    # 執行檢索 (固定使用 k=5)\n",
        "                    print(f\"  🔍 執行檢索: query='{query}'\")\n",
        "                    chunks = search_chunks(query, index, metadata, client, k=5)\n",
        "\n",
        "                    # 格式化結果\n",
        "                    formatted_result = format_chunks_for_llm(chunks)\n",
        "\n",
        "                    print(f\"  ✓ 找到 {len(chunks)} 個相關文件片段\")\n",
        "\n",
        "                    # 加入 function_call_output\n",
        "                    input_messages.append({\n",
        "                        \"type\": \"function_call_output\",\n",
        "                        \"call_id\": call_id,\n",
        "                        \"output\": formatted_result\n",
        "                    })\n",
        "\n",
        "        # 如果沒有 tool calls,結束循環\n",
        "        if not has_tool_calls:\n",
        "            print(\"  ✓ GPT-5 完成回答,無需更多工具\")\n",
        "            break\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    if iteration >= MAX_TOOL_ITERATIONS:\n",
        "        print(f\"⚠️  警告: 達到最大迭代次數 ({MAX_TOOL_ITERATIONS})\")\n",
        "\n",
        "    # 4. 提取最終回答\n",
        "    final_answer = response.output_text\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "\n",
        "# ===== 5. Gradio 介面 =====\n",
        "\n",
        "def create_chatbot_interface(index: faiss.Index, metadata: List[Dict],\n",
        "                            client: OpenAI) -> gr.Interface:\n",
        "    \"\"\"\n",
        "    建立 Gradio chatbot 介面\n",
        "\n",
        "    Args:\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Gradio 介面\n",
        "    \"\"\"\n",
        "    def chat_wrapper(message: str, history: List[Tuple[str, str]]) -> str:\n",
        "        \"\"\"Gradio ChatInterface 的 wrapper\"\"\"\n",
        "        return chat_with_rag(message, history, index, metadata, client)\n",
        "\n",
        "    # 建立 ChatInterface\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=chat_wrapper,\n",
        "        title=\"🤖 GPT-5 API 程式設計助手\",\n",
        "        description=\"\"\"\n",
        "我可以協助你學習和使用 **GPT-5 Response API**!\n",
        "\n",
        "**我的專長領域:**\n",
        "- Function calling 和 custom tools 的使用\n",
        "- Reasoning effort 控制 (minimal, low, medium, high)\n",
        "- Text verbosity 設定\n",
        "- Tool calling 最佳實踐\n",
        "- 程式碼範例和實作建議\n",
        "\n",
        "**提示:** 我會自動搜尋官方文件來回答你的問題,你可以問我任何關於 GPT-5 API 的技術細節!\n",
        "        \"\"\".strip(),\n",
        "        examples=[\n",
        "            \"如何使用 custom tools?\",\n",
        "            \"function calling 的完整流程是什麼?\",\n",
        "            \"reasoning effort 的 minimal 和 low 有什麼差別?\",\n",
        "            \"請給我一個 tool calling loop 的程式碼範例\",\n",
        "            \"如何設定 verbosity 來控制輸出長度?\"\n",
        "        ],\n",
        "        theme=gr.themes.Soft()\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "# ===== 6. 主程式執行 =====\n",
        "\n",
        "def main():\n",
        "    \"\"\"主程式進入點\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🚀 GPT-5 API 程式設計助手 - Part B: RAG 對話機器人\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # 1. 環境偵測與初始化\n",
        "        print(\"\\n[1/4] 環境設定\")\n",
        "        api_key = setup_environment()\n",
        "\n",
        "        # 2. 下載 FAISS (僅 Colab)\n",
        "        print(\"\\n[2/4] FAISS 資料庫準備\")\n",
        "        download_and_extract_faiss()\n",
        "\n",
        "        # 3. 載入向量資料庫\n",
        "        print(\"\\n[3/4] 載入向量資料庫\")\n",
        "        index, metadata, client = load_vectorstore()\n",
        "\n",
        "        # 4. 啟動 Gradio\n",
        "        print(\"\\n[4/4] 啟動 Gradio 介面\")\n",
        "        demo = create_chatbot_interface(index, metadata, client)\n",
        "\n",
        "        print(\"\\n✨ 系統準備完成!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 啟動介面\n",
        "        demo.launch(\n",
        "            share=True,  # 產生 share link (Colab 需要)\n",
        "            server_port=7860,\n",
        "            show_error=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ 錯誤: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
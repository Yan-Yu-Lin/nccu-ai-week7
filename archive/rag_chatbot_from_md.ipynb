{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ - Part B: RAG å°è©±æ©Ÿå™¨äºº\n",
        "\n",
        "æœ¬åœ°åŸ·è¡Œ:\n",
        "    cd Week7\n",
        "    uv run python rag_chatbot.py\n",
        "\n",
        "Google Colab åŸ·è¡Œ:\n",
        "    # 1. å®‰è£å¥—ä»¶\n",
        "    !pip install openai faiss-cpu numpy gradio\n",
        "\n",
        "    # 2. ä¸‹è¼‰å‘é‡è³‡æ–™åº«\n",
        "    GDRIVE_LINK = \"YOUR_GDRIVE_LINK_HERE\"\n",
        "    !gdown --fuzzy {GDRIVE_LINK}\n",
        "    !unzip -o faiss_db.zip\n",
        "\n",
        "    # 3. è¨­å®š API key\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    # 4. åŸ·è¡Œç¨‹å¼\n",
        "    !python rag_chatbot.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# ===== å…¨åŸŸè¨­å®š =====\n",
        "GDRIVE_FILE_ID = \"YOUR_FILE_ID_HERE\"  # å¾…å¡«å…¥\n",
        "FAISS_DB_PATH = \"faiss_db\"\n",
        "MAX_TOOL_ITERATIONS = 5\n",
        "\n",
        "# System prompt for the chatbot\n",
        "SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ GPT-5 Response API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ã€‚\n",
        "\n",
        "ä½ çš„å°ˆé•·é ˜åŸŸ:\n",
        "- GPT-5 Response API çš„ä½¿ç”¨æ–¹å¼\n",
        "- Function calling å’Œ custom tools\n",
        "- Reasoning effort æ§åˆ¶ (minimal, low, medium, high)\n",
        "- Text verbosity è¨­å®š\n",
        "- Tool calling çš„æœ€ä½³å¯¦è¸\n",
        "\n",
        "å›ç­”é¢¨æ ¼:\n",
        "- å‹å–„ã€å°ˆæ¥­ã€æœ‰è€å¿ƒ\n",
        "- æä¾›æ¸…æ¥šçš„ç¨‹å¼ç¢¼ç¯„ä¾‹\n",
        "- è§£é‡‹æŠ€è¡“æ¦‚å¿µæ™‚ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼\n",
        "- å¦‚æœä¸ç¢ºå®šç­”æ¡ˆ,èª å¯¦å‘ŠçŸ¥ä¸¦å»ºè­°æŸ¥é–±å®˜æ–¹æ–‡ä»¶\n",
        "- æ‰€æœ‰ç¨‹å¼ç¢¼ç¯„ä¾‹æˆ–ä»»ä½•èˆ‡ç¨‹å¼ç¢¼ç›¸é—œçš„å…§å®¹éƒ½å¿…é ˆæ”¾åœ¨ code block ä¸­,å¦å‰‡ä¸æ˜“é–±è®€\n",
        "\n",
        "é‡è¦: ç•¶ä½ éœ€è¦æŸ¥è©¢ GPT-5 API ç›¸é—œè³‡æ–™æ™‚,è«‹ä½¿ç”¨ search_chunks å·¥å…·æœå°‹æ–‡ä»¶å…§å®¹ã€‚\"\"\"\n",
        "\n",
        "\n",
        "# ===== 1. ç’°å¢ƒåµæ¸¬èˆ‡åˆå§‹åŒ– =====\n",
        "\n",
        "def is_colab() -> bool:\n",
        "    \"\"\"åµæ¸¬æ˜¯å¦åœ¨ Google Colab ç’°å¢ƒ\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def setup_environment() -> str:\n",
        "    \"\"\"\n",
        "    è¨­å®šç’°å¢ƒä¸¦å–å¾— API key\n",
        "\n",
        "    Returns:\n",
        "        str: OpenAI API key\n",
        "    \"\"\"\n",
        "    if is_colab():\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "        print(\"âœ“ Colab ç’°å¢ƒ: å¾ userdata è¼‰å…¥ API key\")\n",
        "    else:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "        print(\"âœ“ æœ¬åœ°ç’°å¢ƒ: å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥ API key\")\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"æœªæ‰¾åˆ° OPENAI_API_KEY,è«‹ç¢ºèªç’°å¢ƒè¨­å®š\")\n",
        "\n",
        "    return api_key\n",
        "\n",
        "\n",
        "def download_and_extract_faiss(gdrive_url: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    åœ¨ Colab ç’°å¢ƒä¸‹è¼‰ä¸¦è§£å£“ç¸® FAISS è³‡æ–™åº«\n",
        "    æœ¬åœ°ç’°å¢ƒè·³éæ­¤æ­¥é©Ÿ\n",
        "\n",
        "    Args:\n",
        "        gdrive_url: Google Drive åˆ†äº«é€£çµ (å¯é¸)\n",
        "    \"\"\"\n",
        "    if not is_colab():\n",
        "        print(\"âœ“ æœ¬åœ°ç’°å¢ƒ: è·³éä¸‹è¼‰,ç›´æ¥ä½¿ç”¨æœ¬åœ° faiss_db\")\n",
        "        return\n",
        "\n",
        "    if not gdrive_url or gdrive_url == \"YOUR_GDRIVE_LINK_HERE\":\n",
        "        print(\"âš ï¸  è­¦å‘Š: å°šæœªè¨­å®š Google Drive é€£çµ\")\n",
        "        print(\"è«‹åœ¨ç¨‹å¼ç¢¼ä¸­è¨­å®š GDRIVE_FILE_ID æˆ–å‚³å…¥ gdrive_url åƒæ•¸\")\n",
        "        return\n",
        "\n",
        "    print(\"â¬‡ï¸  Colab ç’°å¢ƒ: ä¸‹è¼‰ faiss_db.zip...\")\n",
        "    os.system(f'gdown --fuzzy \"{gdrive_url}\"')\n",
        "\n",
        "    print(\"ğŸ“¦ è§£å£“ç¸® faiss_db.zip...\")\n",
        "    os.system('unzip -o faiss_db.zip')\n",
        "\n",
        "    print(\"âœ“ FAISS è³‡æ–™åº«æº–å‚™å®Œæˆ\")\n",
        "\n",
        "\n",
        "# ===== 2. FAISS è¼‰å…¥ =====\n",
        "\n",
        "def load_vectorstore() -> Tuple[faiss.Index, List[Dict], OpenAI]:\n",
        "    \"\"\"\n",
        "    è¼‰å…¥ FAISS å‘é‡è³‡æ–™åº«\n",
        "\n",
        "    Returns:\n",
        "        Tuple[faiss.Index, List[Dict], OpenAI]: (FAISS index, metadata, OpenAI client)\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“‚ è¼‰å…¥å‘é‡è³‡æ–™åº«: {FAISS_DB_PATH}/\")\n",
        "\n",
        "    # è¼‰å…¥ FAISS index\n",
        "    index_path = os.path.join(FAISS_DB_PATH, \"index.faiss\")\n",
        "    if not os.path.exists(index_path):\n",
        "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ° FAISS index: {index_path}\")\n",
        "\n",
        "    index = faiss.read_index(index_path)\n",
        "    print(f\"  âœ“ FAISS index è¼‰å…¥å®Œæˆ: {index.ntotal} å€‹å‘é‡\")\n",
        "\n",
        "    # è¼‰å…¥ metadata\n",
        "    metadata_path = os.path.join(FAISS_DB_PATH, \"metadata.json\")\n",
        "    if not os.path.exists(metadata_path):\n",
        "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ° metadata: {metadata_path}\")\n",
        "\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"  âœ“ Metadata è¼‰å…¥å®Œæˆ: {len(metadata)} å€‹ chunks\")\n",
        "\n",
        "    # åˆå§‹åŒ– OpenAI client\n",
        "    client = OpenAI()\n",
        "\n",
        "    return index, metadata, client\n",
        "\n",
        "\n",
        "# ===== 3. RAG æª¢ç´¢å‡½æ•¸ =====\n",
        "\n",
        "def search_chunks(query: str, index: faiss.Index, metadata: List[Dict],\n",
        "                 client: OpenAI, k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ FAISS æœå°‹èˆ‡å•é¡Œæœ€ç›¸é—œçš„ chunks\n",
        "\n",
        "    Args:\n",
        "        query: ä½¿ç”¨è€…å•é¡Œ\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "        k: å›å‚³çµæœæ•¸é‡\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: ç›¸é—œçš„ chunks\n",
        "    \"\"\"\n",
        "    # å°‡ query è½‰æˆ embedding\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-large\",\n",
        "        input=[query]\n",
        "    )\n",
        "    query_embedding = response.data[0].embedding\n",
        "\n",
        "    # FAISS æœå°‹\n",
        "    query_vector = np.array([query_embedding]).astype('float32')\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "\n",
        "    # å–å¾—ç›¸é—œ chunks\n",
        "    results = []\n",
        "    for idx, distance in zip(indices[0], distances[0]):\n",
        "        if idx < len(metadata):\n",
        "            chunk = metadata[idx].copy()\n",
        "            chunk['distance'] = float(distance)\n",
        "            results.append(chunk)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def format_chunks_for_llm(chunks: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    æ ¼å¼åŒ– chunks æˆé©åˆ LLM é–±è®€çš„æ ¼å¼\n",
        "\n",
        "    Args:\n",
        "        chunks: æœå°‹çµæœ chunks\n",
        "\n",
        "    Returns:\n",
        "        str: æ ¼å¼åŒ–å¾Œçš„æ–‡å­—\n",
        "    \"\"\"\n",
        "    formatted_parts = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        source = chunk.get('source_file', 'Unknown')\n",
        "        text = chunk.get('text', '')\n",
        "\n",
        "        formatted_parts.append(f\"[æ–‡ä»¶ {i}: {source}]\\n{text}\")\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(formatted_parts)\n",
        "\n",
        "\n",
        "# ===== 4. GPT-5 å°è©±å¼•æ“ =====\n",
        "\n",
        "def convert_gradio_history_to_openai(history: List[Tuple[str, str]]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    è½‰æ› Gradio history æ ¼å¼ç‚º OpenAI messages æ ¼å¼\n",
        "\n",
        "    Args:\n",
        "        history: Gradio history [(user_msg, bot_msg), ...]\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: OpenAI messages æ ¼å¼\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "\n",
        "    for user_msg, bot_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "def chat_with_rag(user_message: str, gradio_history: List[Tuple[str, str]],\n",
        "                 index: faiss.Index, metadata: List[Dict],\n",
        "                 client: OpenAI) -> str:\n",
        "    \"\"\"\n",
        "    ä¸»è¦å°è©±å‡½æ•¸,æ•´åˆ RAG æª¢ç´¢å’Œ GPT-5 å›ç­”\n",
        "\n",
        "    Args:\n",
        "        user_message: ä½¿ç”¨è€…è¼¸å…¥\n",
        "        gradio_history: Gradio å°è©±æ­·å²\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "\n",
        "    Returns:\n",
        "        str: AI å›ç­”\n",
        "    \"\"\"\n",
        "    # 1. è½‰æ›å°è©±æ­·å²\n",
        "    input_messages = convert_gradio_history_to_openai(gradio_history)\n",
        "    input_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # 2. å®šç¾© search_chunks tool\n",
        "    tools = [{\n",
        "        \"type\": \"function\",\n",
        "        \"name\": \"search_chunks\",\n",
        "        \"description\": \"æœå°‹ GPT-5 Response API å®˜æ–¹æ–‡ä»¶å…§å®¹ã€‚ç•¶ä½¿ç”¨è€…è©¢å•é—œæ–¼ GPT-5 APIã€function callingã€tool callingã€reasoning effortã€verbosity ç­‰æŠ€è¡“å•é¡Œæ™‚,è«‹ä½¿ç”¨æ­¤å·¥å…·æœå°‹ç›¸é—œæ–‡ä»¶ã€‚\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"æœå°‹é—œéµå­—æˆ–å•é¡Œ,ä¾‹å¦‚: 'custom tools', 'reasoning effort', 'function calling example'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "            \"additionalProperties\": False\n",
        "        },\n",
        "        \"strict\": True\n",
        "    }]\n",
        "\n",
        "    # 3. Tool calling loop\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < MAX_TOOL_ITERATIONS:\n",
        "        print(f\"\\nğŸ”„ Tool calling iteration {iteration + 1}\")\n",
        "\n",
        "        # å‘¼å« GPT-5\n",
        "        response = client.responses.create(\n",
        "            model=\"gpt-5\",\n",
        "            reasoning={\"effort\": \"low\"},\n",
        "            instructions=SYSTEM_PROMPT,\n",
        "            input=input_messages,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        # å°‡ response.output çš„æ¯å€‹ item åŠ åˆ° input_messages\n",
        "        # é‡è¦: ä¸è¦ç›´æ¥ += response.outputï¼Œè¦é€å€‹ append\n",
        "        for item in response.output:\n",
        "            input_messages.append(item)\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦æœ‰ function calls ä¸¦åŸ·è¡Œ\n",
        "        has_tool_calls = False\n",
        "\n",
        "        for item in response.output:\n",
        "            if item.type == \"function_call\":\n",
        "                has_tool_calls = True\n",
        "                function_name = item.name\n",
        "                call_id = item.call_id\n",
        "\n",
        "                print(f\"  ğŸ“ GPT-5 å‘¼å«å·¥å…·: {function_name}\")\n",
        "                print(f\"     åƒæ•¸: {item.arguments}\")\n",
        "\n",
        "                if function_name == \"search_chunks\":\n",
        "                    # è§£æåƒæ•¸\n",
        "                    args = json.loads(item.arguments)\n",
        "                    query = args.get(\"query\", \"\")\n",
        "\n",
        "                    # åŸ·è¡Œæª¢ç´¢ (å›ºå®šä½¿ç”¨ k=5)\n",
        "                    print(f\"  ğŸ” åŸ·è¡Œæª¢ç´¢: query='{query}'\")\n",
        "                    chunks = search_chunks(query, index, metadata, client, k=5)\n",
        "\n",
        "                    # æ ¼å¼åŒ–çµæœ\n",
        "                    formatted_result = format_chunks_for_llm(chunks)\n",
        "\n",
        "                    print(f\"  âœ“ æ‰¾åˆ° {len(chunks)} å€‹ç›¸é—œæ–‡ä»¶ç‰‡æ®µ\")\n",
        "\n",
        "                    # åŠ å…¥ function_call_output\n",
        "                    input_messages.append({\n",
        "                        \"type\": \"function_call_output\",\n",
        "                        \"call_id\": call_id,\n",
        "                        \"output\": formatted_result\n",
        "                    })\n",
        "\n",
        "        # å¦‚æœæ²’æœ‰ tool calls,çµæŸå¾ªç’°\n",
        "        if not has_tool_calls:\n",
        "            print(\"  âœ“ GPT-5 å®Œæˆå›ç­”,ç„¡éœ€æ›´å¤šå·¥å…·\")\n",
        "            break\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    if iteration >= MAX_TOOL_ITERATIONS:\n",
        "        print(f\"âš ï¸  è­¦å‘Š: é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({MAX_TOOL_ITERATIONS})\")\n",
        "\n",
        "    # 4. æå–æœ€çµ‚å›ç­”\n",
        "    final_answer = response.output_text\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "\n",
        "# ===== 5. Gradio ä»‹é¢ =====\n",
        "\n",
        "def create_chatbot_interface(index: faiss.Index, metadata: List[Dict],\n",
        "                            client: OpenAI) -> gr.Interface:\n",
        "    \"\"\"\n",
        "    å»ºç«‹ Gradio chatbot ä»‹é¢\n",
        "\n",
        "    Args:\n",
        "        index: FAISS index\n",
        "        metadata: Chunk metadata\n",
        "        client: OpenAI client\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Gradio ä»‹é¢\n",
        "    \"\"\"\n",
        "    def chat_wrapper(message: str, history: List[Tuple[str, str]]) -> str:\n",
        "        \"\"\"Gradio ChatInterface çš„ wrapper\"\"\"\n",
        "        return chat_with_rag(message, history, index, metadata, client)\n",
        "\n",
        "    # å»ºç«‹ ChatInterface\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=chat_wrapper,\n",
        "        title=\"ğŸ¤– GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹\",\n",
        "        description=\"\"\"\n",
        "æˆ‘å¯ä»¥å”åŠ©ä½ å­¸ç¿’å’Œä½¿ç”¨ **GPT-5 Response API**!\n",
        "\n",
        "**æˆ‘çš„å°ˆé•·é ˜åŸŸ:**\n",
        "- Function calling å’Œ custom tools çš„ä½¿ç”¨\n",
        "- Reasoning effort æ§åˆ¶ (minimal, low, medium, high)\n",
        "- Text verbosity è¨­å®š\n",
        "- Tool calling æœ€ä½³å¯¦è¸\n",
        "- ç¨‹å¼ç¢¼ç¯„ä¾‹å’Œå¯¦ä½œå»ºè­°\n",
        "\n",
        "**æç¤º:** æˆ‘æœƒè‡ªå‹•æœå°‹å®˜æ–¹æ–‡ä»¶ä¾†å›ç­”ä½ çš„å•é¡Œ,ä½ å¯ä»¥å•æˆ‘ä»»ä½•é—œæ–¼ GPT-5 API çš„æŠ€è¡“ç´°ç¯€!\n",
        "        \"\"\".strip(),\n",
        "        examples=[\n",
        "            \"å¦‚ä½•ä½¿ç”¨ custom tools?\",\n",
        "            \"function calling çš„å®Œæ•´æµç¨‹æ˜¯ä»€éº¼?\",\n",
        "            \"reasoning effort çš„ minimal å’Œ low æœ‰ä»€éº¼å·®åˆ¥?\",\n",
        "            \"è«‹çµ¦æˆ‘ä¸€å€‹ tool calling loop çš„ç¨‹å¼ç¢¼ç¯„ä¾‹\",\n",
        "            \"å¦‚ä½•è¨­å®š verbosity ä¾†æ§åˆ¶è¼¸å‡ºé•·åº¦?\"\n",
        "        ],\n",
        "        theme=gr.themes.Soft()\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "# ===== 6. ä¸»ç¨‹å¼åŸ·è¡Œ =====\n",
        "\n",
        "def main():\n",
        "    \"\"\"ä¸»ç¨‹å¼é€²å…¥é»\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸš€ GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ - Part B: RAG å°è©±æ©Ÿå™¨äºº\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # 1. ç’°å¢ƒåµæ¸¬èˆ‡åˆå§‹åŒ–\n",
        "        print(\"\\n[1/4] ç’°å¢ƒè¨­å®š\")\n",
        "        api_key = setup_environment()\n",
        "\n",
        "        # 2. ä¸‹è¼‰ FAISS (åƒ… Colab)\n",
        "        print(\"\\n[2/4] FAISS è³‡æ–™åº«æº–å‚™\")\n",
        "        download_and_extract_faiss()\n",
        "\n",
        "        # 3. è¼‰å…¥å‘é‡è³‡æ–™åº«\n",
        "        print(\"\\n[3/4] è¼‰å…¥å‘é‡è³‡æ–™åº«\")\n",
        "        index, metadata, client = load_vectorstore()\n",
        "\n",
        "        # 4. å•Ÿå‹• Gradio\n",
        "        print(\"\\n[4/4] å•Ÿå‹• Gradio ä»‹é¢\")\n",
        "        demo = create_chatbot_interface(index, metadata, client)\n",
        "\n",
        "        print(\"\\nâœ¨ ç³»çµ±æº–å‚™å®Œæˆ!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # å•Ÿå‹•ä»‹é¢\n",
        "        demo.launch(\n",
        "            share=True,  # ç”¢ç”Ÿ share link (Colab éœ€è¦)\n",
        "            server_port=7860,\n",
        "            show_error=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ éŒ¯èª¤: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
[
  {
    "chunk_id": "call_ikQLWd48AoatJzHgPWz0UyhZ",
    "text": "Using GPT-5\n===========\n\nLearn best practices, features, and migration guidance for GPT-5.\n\nGPT-5 is our most intelligent model yet, trained to be especially proficient in:\n\n*   Code generation, bug fixing, and refactoring\n*   Instruction following\n*   Long context and tool calling\n\nThis guide covers key features of the GPT-5 model family and how to get the most out of GPT-5.\n\n### Explore coding examples\n\nClick through a few demo applications generated entirely with a single GPT-5 prompt, without writing any code by hand.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 0,
    "char_count": 527
  },
  {
    "chunk_id": "call_QM119Zb45233gyO2suIY9bsX",
    "text": "Quickstart\n----------\n\nFaster responses\n\nBy default, GPT-5 produces a medium length chain of thought before responding to a prompt. For faster, lower-latency responses, use low reasoning effort and low text verbosity.  \n  \nThis behavior will more closely (but not exactly!) match non-reasoning models like [GPT-4.1](/docs/models/gpt-4.1). We expect GPT-5 to produce more intelligent responses than GPT-4.1, but when speed and maximum context length are paramount, you might consider using GPT-4.1 instead.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 1,
    "char_count": 505
  },
  {
    "chunk_id": "call_aQgBzKxI6LUIhQdcqMxC6aRk",
    "text": "Fast, low latency response options\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresult = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Write a haiku about code.\",\n    reasoning={ \"effort\": \"low\" },\n    text={ \"verbosity\": \"low\" },\n)\n\nprint(result.output_text)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 2,
    "char_count": 284
  },
  {
    "chunk_id": "call_faOWtXi6J48qzCgUcm0s8jlP",
    "text": "Coding and agentic tasks\n\nGPT-5 is great at reasoning through complex tasks. **For complex tasks like coding and multi-step planning, use high reasoning effort.**  \n  \nUse these configurations when replacing tasks you might have used o3 to tackle. We expect GPT-5 to produce better results than o3 and o4-mini under most circumstances.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 3,
    "char_count": 335
  },
  {
    "chunk_id": "call_oZ25ZHZTxgaUXeLDVfj2fHrQ",
    "text": "Slower, high reasoning tasks\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresult = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Find the null pointer exception: ...your code here...\",\n    reasoning={ \"effort\": \"high\" },\n)\n\nprint(result.output_text)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 4,
    "char_count": 274
  },
  {
    "chunk_id": "call_PkoDcC2TWto4dvM6PnrSk4zR",
    "text": "Meet the models\n---------------\n\nThere are three models in the GPT-5 series. In general, `gpt-5` is best for your most complex tasks that require broad world knowledge. The smaller mini and nano models trade off some general world knowledge for lower cost and lower latency. Small models will tend to perform better for more well defined tasks.\n\nTo help you pick the model that best fits your use case, consider these tradeoffs:\n\n|Variant|Best for|\n|---|---|\n|gpt-5|Complex reasoning, broad world knowledge, and code-heavy or multi-step agentic tasks|\n|gpt-5-mini|Cost-optimized reasoning and chat; balances speed, cost, and capability|\n|gpt-5-nano|High-throughput tasks, especially simple instruction-following or classification|",
    "source_file": "Using GPT 5.md",
    "chunk_index": 5,
    "char_count": 730
  },
  {
    "chunk_id": "call_Xhg3JxYXPP0fHCIq6uDUMTSn",
    "text": "### Model name reference\n\nThe GPT-5 [system card](https://openai.com/index/gpt-5-system-card/) uses different names than the API. Use this table to map between them:\n\n|System card name|API alias|\n|---|---|\n|gpt-5-thinking|gpt-5|\n|gpt-5-thinking-mini|gpt-5-mini|\n|gpt-5-thinking-nano|gpt-5-nano|\n|gpt-5-main|gpt-5-chat-latest|\n|gpt-5-main-mini|[not available via API]|",
    "source_file": "Using GPT 5.md",
    "chunk_index": 6,
    "char_count": 367
  },
  {
    "chunk_id": "call_8cEvDUeZjK8z9mJ6sDSae8zq",
    "text": "### New API features in GPT-5\n\nAlongside GPT-5, we're introducing a few new parameters and API features designed to give developers more control and flexibility: the ability to control verbosity, a minimal reasoning effort option, custom tools, and an allowed tools list.\n\nThis guide walks through some of the key features of the GPT-5 model family and how to get the most out of these models.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 7,
    "char_count": 393
  },
  {
    "chunk_id": "call_g5mXK1cI2c95Bq5ped3GN7Ts",
    "text": "Minimal reasoning effort\n------------------------\n\nThe `reasoning.effort` parameter controls how many reasoning tokens the model generates before producing a response. Earlier reasoning models like o3 supported only `low`, `medium`, and `high`: `low` favored speed and fewer tokens, while `high` favored more thorough reasoning.\n\nThe new `minimal` setting produces very few reasoning tokens for cases where you need the fastest possible time-to-first-token. We often see better performance when the model can produce a few tokens when needed versus none. The default is `medium`.\n\nThe `minimal` setting performs especially well in coding and instruction following scenarios, adhering closely to given directions. However, it may require prompting to act more proactively. To improve the model's reasoning quality, even at minimal effort, encourage it to “think” or outline its steps before answering.\n\nMinimal reasoning effort\n\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\",\n    reasoning={\n        \"effort\": \"minimal\"\n    }\n)\n\nprint(response)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 8,
    "char_count": 1199
  },
  {
    "chunk_id": "call_mkIfBMsIbPXNyX6dVsGC04M0",
    "text": "### Verbosity\n\nVerbosity determines how many output tokens are generated. Lowering the number of tokens reduces overall latency. While the model's reasoning approach stays mostly the same, the model finds ways to answer more concisely—which can either improve or diminish answer quality, depending on your use case. Here are some scenarios for both ends of the verbosity spectrum:\n\n*   **High verbosity:** Use when you need the model to provide thorough explanations of documents or perform extensive code refactoring.\n*   **Low verbosity:** Best for situations where you want concise answers or simple code generation, such as SQL queries.\n\nModels before GPT-5 have used `medium` verbosity by default. With GPT-5, we make this option configurable as one of `high`, `medium`, or `low`.\n\nWhen generating code, `medium` and `high` verbosity levels yield longer, more structured code with inline explanations, while `low` verbosity produces shorter, more concise code with minimal commentary.\n\nControl verbosity\n\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n    text={\n        \"verbosity\": \"low\"\n    }\n)\n\nprint(response)\n```\n\nYou can still steer verbosity through prompting after setting it to `low` in the API. The verbosity parameter defines a general token range at the system prompt level, but the actual output is flexible to both developer and user prompts within that range.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 9,
    "char_count": 1541
  },
  {
    "chunk_id": "call_gSOYaooyYnf2UDC4A2VZj0H5",
    "text": "### Custom tools\n\nWith GPT-5, we're introducing a new capability called custom tools, which lets models send any raw text as tool call input but still constrain outputs if desired.\n\n[\n\nFunction calling guide\n\nLearn about custom tools in the function calling guide.\n\n](/docs/guides/function-calling)\n\n#### Freeform inputs\n\nDefine your tool with `type: custom` to enable models to send plaintext inputs directly to your tools, rather than being limited to structured JSON. The model can send any raw text—code, SQL queries, shell commands, configuration files, or long-form prose—directly to your tool.\n\n\n#### Constraining outputs\n\nGPT-5 supports context-free grammars (CFGs) for custom tools, letting you provide a Lark grammar to constrain outputs to a specific syntax or DSL. Attaching a CFG (e.g., a SQL or DSL grammar) ensures the assistant's text matches your grammar.\n\nThis enables precise, constrained tool calls or structured responses and lets you enforce strict syntactic or domain-specific formats directly in GPT-5's function calling, improving control and reliability for complex or constrained domains.\n\n#### Best practices for custom tools\n\n*   **Write concise, explicit tool descriptions**. The model chooses what to send based on your description; state clearly if you want it to always call the tool.\n*   **Validate outputs on the server side**. Freeform strings are powerful but require safeguards against injection or unsafe commands.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 10,
    "char_count": 1453
  },
  {
    "chunk_id": "call_aeuXARcTshC6unrLhZaTuo4u",
    "text": "### Allowed tools\n\nThe `allowed_tools` parameter under `tool_choice` lets you pass N tool definitions but restrict the model to only M (< N) of them. List your full toolkit in `tools`, and then use an `allowed_tools` block to name the subset and specify a mode—either `auto` (the model may pick any of those) or `required` (the model must invoke one).\n\n[\n\nFunction calling guide\n\nLearn about the allowed tools option in the function calling guide.\n\n](/docs/guides/function-calling)\n\nBy separating all possible tools from the subset that can be used _now_, you gain greater safety, predictability, and improved prompt caching. You also avoid brittle prompt engineering, such as hard-coded call order. GPT-5 dynamically invokes or requires specific functions mid-conversation while reducing the risk of unintended tool usage over long contexts.\n\n||Standard Tools|Allowed Tools|\n|---|---|---|\n|Model's universe|All tools listed under \"tools\": […]|Only the subset under \"tools\": […] in tool_choice|\n|Tool invocation|Model may or may not call any tool|Model restricted to (or required to call) chosen tools|\n|Purpose|Declare available capabilities|Constrain which capabilities are actually used|\n\n\nFor a more detailed overview of all of these new features, see the [accompanying cookbook](https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 11,
    "char_count": 1355
  },
  {
    "chunk_id": "call_65Oy7tw7Prh4bHFfjn72zXPw",
    "text": "### Preambles\n\nPreambles are brief, user-visible explanations that GPT-5 generates before invoking any tool or function, outlining its intent or plan (e.g., “why I'm calling this tool”). They appear after the chain-of-thought and before the actual tool call, providing transparency into the model's reasoning and enhancing debuggability, user confidence, and fine-grained steerability.\n\nBy letting GPT-5 “think out loud” before each tool call, preambles boost tool-calling accuracy (and overall task success) without bloating reasoning overhead. To enable preambles, add a system or developer instruction—for example: “Before you call a tool, explain why you are calling it.” GPT-5 prepends a concise rationale to each specified tool call. The model may also output multiple messages between tool calls, which can enhance the interaction experience—particularly for minimal reasoning or latency-sensitive use cases.\n\nFor more on using preambles, see the [GPT-5 prompting cookbook](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide#tool-preambles).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 12,
    "char_count": 1062
  },
  {
    "chunk_id": "call_QsEah2u7r7XZMp2hEGw0DujL",
    "text": "Migration guidance\n------------------\n\nGPT-5 is our best model yet, and it works best with the Responses API, which supports for passing chain of thought (CoT) between turns. Read below to migrate from your current model or API.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 13,
    "char_count": 228
  },
  {
    "chunk_id": "call_WNEyBjN25kxA5TKXGX0wNjFT",
    "text": "### Migrating from other models to GPT-5\n\nWe see improved intelligence because the Responses API can pass the previous turn's CoT to the model. This leads to fewer generated reasoning tokens, higher cache hit rates, and less latency. To learn more, see an [in-depth guide](https://cookbook.openai.com/examples/responses_api/reasoning_items) on the benefits of responses.\n\nWhen migrating to GPT-5 from an older OpenAI model, start by experimenting with reasoning levels and prompting strategies. Based on our testing, we recommend using our [prompt optimizer](http://platform.openai.com/chat/edit?optimize=true)—which automatically updates your prompts for GPT-5 based on our best practices—and following this model-specific guidance:\n\n*   **o3**: `gpt-5` with `medium` or `high` reasoning is a great replacement. Start with `medium` reasoning with prompt tuning, then increasing to `high` if you aren't getting the results you want.\n*   **gpt-4.1**: `gpt-5` with `minimal` or `low` reasoning is a strong alternative. Start with `minimal` and tune your prompts; increase to `low` if you need better performance.\n*   **o4-mini or gpt-4.1-mini**: `gpt-5-mini` with prompt tuning is a great replacement.\n*   **gpt-4.1-nano**: `gpt-5-nano` with prompt tuning is a great replacement.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 14,
    "char_count": 1277
  },
  {
    "chunk_id": "call_AasxNRMvB0Tjs6lVRboTyypG",
    "text": "### GPT-5 parameter compatibility\n\n⚠️ **Important:** The following parameters are **not supported** when using GPT-5 models (e.g. `gpt-5`, `gpt-5-mini`, `gpt-5-nano`):\n\n*   `temperature`\n*   `top_p`\n*   `logprobs`\n\nRequests that include these fields will raise an error.\n\n**Instead, use the following GPT-5-specific controls:**\n\n*   **Reasoning depth:** `reasoning: { effort: \"minimal\" | \"low\" | \"medium\" | \"high\" }`\n*   **Output verbosity:** `text: { verbosity: \"low\" | \"medium\" | \"high\" }`\n*   **Output length:** `max_output_tokens`",
    "source_file": "Using GPT 5.md",
    "chunk_index": 15,
    "char_count": 534
  },
  {
    "chunk_id": "call_PWVFhLillgRoKRIqbMV005vR",
    "text": "### Migrating from Chat Completions to Responses API\n\nThe biggest difference, and main reason to migrate from Chat Completions to the Responses API for GPT-5, is support for passing chain of thought (CoT) between turns. See a full [comparison of the APIs](/docs/guides/responses-vs-chat-completions).\n\nPassing CoT exists only in the Responses API, and we've seen improved intelligence, fewer generated reasoning tokens, higher cache hit rates, and lower latency as a result of doing so. Most other parameters remain at parity, though the formatting is different. Here's how new parameters are handled differently between Chat Completions and the Responses API:\n\n**Reasoning effort**\n\nResponses API\n\nGenerate response with minimal reasoning\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/responses \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\",\n  \"reasoning\": {\n    \"effort\": \"minimal\"\n  }\n}'\n```\n\nChat Completions\n\nGenerate response with minimal reasoning\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/chat/completions \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\"\n    }\n  ],\n  \"reasoning_effort\": \"minimal\"\n}'\n```\n\n**Verbosity**\n\nResponses API\n\nControl verbosity\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/responses \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"What is the answer to the ultimate question of life, the universe, and everything?\",\n  \"text\": {\n    \"verbosity\": \"low\"\n  }\n}'\n```\n\nChat Completions\n\nControl verbosity\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/chat/completions \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"What is the answer to the ultimate question of life, the universe, and everything?\" }\n  ],\n  \"verbosity\": \"low\"\n}'\n```\n\n**Custom tools**\n\nResponses API\n\nCustom tool call\n\n```json\ncurl --request POST --url https://api.openai.com/v1/responses --header \"Authorization: Bearer $OPENAI_API_KEY\" --header 'Content-type: application/json' --data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry\",\n  \"tools\": [\n    {\n      \"type\": \"custom\",\n      \"name\": \"code_exec\",\n      \"description\": \"Executes arbitrary python code\"\n    }\n  ]\n}'\n```\n\nChat Completions\n\nCustom tool call\n\n```json\ncurl --request POST --url https://api.openai.com/v1/chat/completions --header \"Authorization: Bearer $OPENAI_API_KEY\" --header 'Content-type: application/json' --data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry\" }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"custom\",\n      \"custom\": {\n        \"name\": \"code_exec\",\n        \"description\": \"Executes arbitrary python code\"\n      }\n    }\n  ]\n}'\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 16,
    "char_count": 3430
  },
  {
    "chunk_id": "call_k7EVdXzLxann8gR2iAUEEByU",
    "text": "Prompting guidance\n------------------\n\nWe specifically designed GPT-5 to excel at coding, frontend engineering, and tool-calling for agentic tasks. We also recommend iterating on prompts for GPT-5 using the [prompt optimizer](/chat/edit?optimize=true).\n\n[\n\nGPT-5 prompt optimizer\n\nCraft the perfect prompt for GPT-5 in the dashboard\n\n](/chat/edit?optimize=true)[\n\nGPT-5 prompting guide\n\nLearn full best practices for prompting GPT-5 models\n\n](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)[\n\nFrontend prompting for GPT-5\n\nSee prompt samples specific to frontend development\n\n](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)",
    "source_file": "Using GPT 5.md",
    "chunk_index": 17,
    "char_count": 654
  },
  {
    "chunk_id": "call_fWxLRSJpIvJN3CA1uXMaglyu",
    "text": "### GPT-5 is a reasoning model\n\nReasoning models like GPT-5 break problems down step by step, producing an internal chain of thought that encodes their reasoning. To maximize performance, pass these reasoning items back to the model: this avoids re-reasoning and keeps interactions closer to the model's training distribution. In multi-turn conversations, passing a `previous_response_id` automatically makes earlier reasoning items available. This is especially important when using tools—for example, when a function call requires an extra round trip. In these cases, either include them with `previous_response_id` or add them directly to `input`.\n\nLearn more about reasoning models and how to get the most out of them in our [reasoning guide](/docs/guides/reasoning).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 18,
    "char_count": 771
  },
  {
    "chunk_id": "call_h9Uo9fVJQqjQRF98NH133NDx",
    "text": "Further reading\n---------------\n\n[GPT-5 prompting guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)\n\n[GPT-5 frontend guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)\n\n[GPT-5 new features guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools)\n\n[Cookbook on reasoning models](https://cookbook.openai.com/examples/responses_api/reasoning_items)\n\n[Comparison of Responses API vs. Chat Completions](/docs/guides/migrate-to-responses)",
    "source_file": "Using GPT 5.md",
    "chunk_index": 19,
    "char_count": 491
  },
  {
    "chunk_id": "call_Qz7KCuxxln4TtBwpLnAm4DO8",
    "text": "FAQ\n---\n\n1.  **How are these models integrated into ChatGPT?**\n    \n    In ChatGPT, there are two models: `gpt-5-chat` and `gpt-5-thinking`. They offer reasoning and minimal-reasoning capabilities, with a routing layer that selects the best model based on the user's question. Users can also invoke reasoning directly through the ChatGPT UI.\n    \n2.  **Will these models be supported in Codex?**\n    \n    Yes, `gpt-5` will be available in Codex and Codex CLI.\n    \n3.  **How does GPT-5 compare to GPT-5-Codex?**\n    \n    [`GPT-5-Codex`](/docs/models/gpt-5-codex) was specifically designed for use in Codex. Unlike `GPT-5`, which is a general-purpose model, we recommend using GPT-5-Codex only for agentic coding tasks in Codex or Codex-like environments, and GPT-5 for use cases in other domains. GPT-5-Codex is only available in the Responses API and supports low, medium, and high `reasoning_efforts` and function calling, structured outputs, and the `web_search` tool.\n    \n4.  **What is the deprecation plan for previous models?**\n    \n    Any model deprecations will be posted on our [deprecations page](/docs/deprecations#page-top). We'll send advanced notice of any model deprecations.\n    \n\nWas this page useful?",
    "source_file": "Using GPT 5.md",
    "chunk_index": 20,
    "char_count": 1220
  }
]
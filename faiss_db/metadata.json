[
  {
    "chunk_id": "call_fGWO0REWQwxTqjmCWZTzwb5z",
    "text": "# 公司客服 Q&A 範例",
    "source_file": "test_company_faq.md",
    "chunk_index": 0,
    "char_count": 13,
    "vector_index": 0,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_16V2gCGU6YgEdtMDVp37axAY",
    "text": "## 退款相關\n\n### Q: 如何申請退款？\nA: 您可以透過以下步驟申請退款：\n1. 登入您的帳戶\n2. 進入「訂單管理」頁面\n3. 選擇要退款的訂單\n4. 點擊「申請退款」按鈕\n5. 填寫退款原因並提交\n\n退款申請會在 3-5 個工作日內處理完成。",
    "source_file": "test_company_faq.md",
    "chunk_index": 1,
    "char_count": 127,
    "vector_index": 1,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_WKRF5MFL26vr3zHfRuH5i2x4",
    "text": "### Q: 退款需要多久才能收到？\nA: 退款處理時間依支付方式而定：\n- 信用卡退款：7-14 個工作日\n- ATM 轉帳退款：3-5 個工作日\n- 超商付款退款：5-7 個工作日\n\n退款金額將退回您原本的付款帳戶。",
    "source_file": "test_company_faq.md",
    "chunk_index": 2,
    "char_count": 110,
    "vector_index": 2,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_ui9kjXBxRXvLzqy2utdYJrdy",
    "text": "### Q: 可以部分退款嗎？\nA: 是的，我們支援部分退款。如果您的訂單包含多個商品，您可以選擇只退款其中幾項商品。但請注意，部分退款可能會影響原本的優惠折扣。",
    "source_file": "test_company_faq.md",
    "chunk_index": 3,
    "char_count": 81,
    "vector_index": 3,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_BqBIuuWniTp1OO1LigONCAjl",
    "text": "## 配送相關\n\n### Q: 配送需要多久時間？\nA: 標準配送時間如下：\n- 台北市區：1-2 個工作日\n- 台灣本島其他地區：2-3 個工作日\n- 離島地區：3-5 個工作日\n\n急件可以選擇快速配送服務，額外收費 NT$ 100。",
    "source_file": "test_company_faq.md",
    "chunk_index": 4,
    "char_count": 118,
    "vector_index": 4,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_zfBa63xUjX8QPqMrAsPZAvw8",
    "text": "### Q: 可以指定配送時間嗎？\nA: 可以的！我們提供以下配送時段：\n- 上午時段（9:00-12:00）\n- 下午時段（13:00-18:00）\n- 晚上時段（18:00-21:00）\n\n請在結帳時選擇您方便的時段。",
    "source_file": "test_company_faq.md",
    "chunk_index": 5,
    "char_count": 111,
    "vector_index": 5,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_gD64q0fXSWh54ZhMcQTfjfj4",
    "text": "### Q: 如果配送地址填錯了怎麼辦？\nA: 如果訂單尚未出貨，您可以：\n1. 聯繫客服修改地址\n2. 或在訂單管理頁面自行修改\n\n如果訂單已經出貨，請聯繫客服協助處理，可能需要支付額外的配送費用。",
    "source_file": "test_company_faq.md",
    "chunk_index": 6,
    "char_count": 100,
    "vector_index": 6,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_g2ugVCuJbP9Wtv07QJvx6uvr",
    "text": "## 產品相關\n\n### Q: 產品有保固嗎？\nA: 所有產品都享有以下保固：\n- 電子產品：1 年保固\n- 家電產品：2 年保固\n- 服飾配件：30 天鑑賞期\n\n保固期間內如有非人為損壞，我們提供免費維修或更換服務。",
    "source_file": "test_company_faq.md",
    "chunk_index": 7,
    "char_count": 109,
    "vector_index": 7,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_C5TBsUn2mCr8FpHCEoSwDIgn",
    "text": "### Q: 如何確認產品真偽？\nA: 每個產品都有獨特的防偽標籤，您可以透過以下方式驗證：\n1. 刮開防偽標籤上的塗層\n2. 取得驗證碼\n3. 至官網「產品驗證」頁面輸入驗證碼\n4. 系統會顯示產品真偽及詳細資訊",
    "source_file": "test_company_faq.md",
    "chunk_index": 8,
    "char_count": 107,
    "vector_index": 8,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_xbtk4E2gRwjqIke4hrEFUNAO",
    "text": "### Q: 可以客製化產品嗎？\nA: 部分產品支援客製化服務，包括：\n- 刻字服務\n- 顏色選擇\n- 尺寸訂製\n\n詳情請參考各產品頁面的客製化選項，或聯繫客服諮詢。",
    "source_file": "test_company_faq.md",
    "chunk_index": 9,
    "char_count": 83,
    "vector_index": 9,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_ureTn8JIkup6oK496PxEub0a",
    "text": "Text generation\n===============\n\nLearn how to prompt a model to generate text.\n\nWith the OpenAI API, you can use a [large language model](/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com). Models can generate almost any kind of text response—like code, mathematical equations, structured JSON data, or human-like prose.\n\nHere's a simple example using the [Responses API](/docs/api-reference/responses), our recommended API for all new projects.\n\nGenerate text from a simple prompt\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Write a one-sentence bedtime story about a unicorn.\"\n)\n\nprint(response.output_text)\n```\n\n```csharp\nusing OpenAI.Responses;\n\nstring key = Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\")!;\nOpenAIResponseClient client = new(model: \"gpt-5\", apiKey: key);\n\nOpenAIResponse response = client.CreateResponse(\n    \"Write a one-sentence bedtime story about a unicorn.\"\n);\n\nConsole.WriteLine(response.GetOutputText());\n```",
    "source_file": "text generation.md",
    "chunk_index": 0,
    "char_count": 1066,
    "vector_index": 10,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_6ehfaEgKs94qGy3SL7dSFvIo",
    "text": "An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:\n\n```json\n[\n    {\n        \"id\": \"msg_67b73f697ba4819183a15cc17d011509\",\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n            {\n                \"type\": \"output_text\",\n                \"text\": \"Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.\",\n                \"annotations\": []\n            }\n        ]\n    }\n]\n```\n\n**The `output` array often has more than one item in it!** It can contain tool calls, data about reasoning tokens generated by [reasoning models](/docs/guides/reasoning), and other items. It is not safe to assume that the model's text output is present at `output[0].content[0].text`.\n\nSome of our [official SDKs](/docs/libraries) include an `output_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.\n\nIn addition to plain text, you can also have the model return structured data in JSON format—this feature is called [**Structured Outputs**](/docs/guides/structured-outputs).",
    "source_file": "text generation.md",
    "chunk_index": 1,
    "char_count": 1320,
    "vector_index": 11,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_rZT2mupPTTM9Oz3mbjfVQzym",
    "text": "Prompt engineering\n------------------\n\n**Prompt engineering** is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.\n\nBecause the content generated from a model is non-deterministic, prompting to get your desired output is a mix of art and science. However, you can apply techniques and best practices to get good results consistently.\n\nSome prompt engineering techniques work with every model, like using message roles. But different models might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you build more complex applications, we strongly recommend:\n\n*   Pinning your production applications to specific [model snapshots](/docs/models) (like `gpt-5-2025-08-07` for example) to ensure consistent behavior\n*   Building [evals](/docs/guides/evals) that measure the behavior of your prompts so you can monitor prompt performance as you iterate, or when you change and upgrade model versions\n\nNow, let's examine some tools and techniques available to you to construct prompts.",
    "source_file": "text generation.md",
    "chunk_index": 2,
    "char_count": 1168,
    "vector_index": 12,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_0nOGS8ylC6IH6ApTMZJKYEQs",
    "text": "Choosing models and APIs\n------------------------\n\nOpenAI has many different [models](/docs/models) and several APIs to choose from. [Reasoning models](/docs/guides/reasoning), like o3 and GPT-5, behave differently from chat models and respond better to different prompts. One important note is that reasoning models perform better and demonstrate higher intelligence when used with the Responses API.\n\nIf you're building any text generation app, we recommend using the Responses API over the older Chat Completions API. And if you're using a reasoning model, it's especially useful to [migrate to Responses](/docs/guides/migrate-to-responses).",
    "source_file": "text generation.md",
    "chunk_index": 3,
    "char_count": 644,
    "vector_index": 13,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_xNWotBEVaXrc2UUbHzSCehDq",
    "text": "Message roles and instruction following\n---------------------------------------\n\nYou can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain_of_command) using the `instructions` API parameter along with **message roles**.\n\nThe `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the `input` parameter.\n\nGenerate text with instructions\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    reasoning={\"effort\": \"low\"},\n    instructions=\"Talk like a pirate.\",\n    input=\"Are semicolons optional in JavaScript?\",\n)\n\nprint(response.output_text)\n```",
    "source_file": "text generation.md",
    "chunk_index": 4,
    "char_count": 872,
    "vector_index": 14,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_nitVCGrjfpmnOtu3IHpX34aX",
    "text": "The example above is roughly equivalent to using the following input messages in the `input` array:\n\nGenerate text with messages using different roles\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    reasoning={\"effort\": \"low\"},\n    input=[\n        {\n            \"role\": \"developer\",\n            \"content\": \"Talk like a pirate.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Are semicolons optional in JavaScript?\"\n        }\n    ]\n)\n\nprint(response.output_text)\n```",
    "source_file": "text generation.md",
    "chunk_index": 5,
    "char_count": 559,
    "vector_index": 15,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_CAgfxyHYra8XLwCRRo98zpjz",
    "text": "Note that the `instructions` parameter only applies to the current response generation request. If you are [managing conversation state](/docs/guides/conversation-state) with the `previous_response_id` parameter, the `instructions` used on previous turns will not be present in the context.\n\nThe [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how our models give different levels of priority to messages with different roles.\n\n|developer|user|assistant|\n|---|---|---|\n|developer messages are instructions provided by the application developer, prioritized ahead of user messages.|user messages are instructions provided by an end user, prioritized behind developer messages.|Messages generated by the model have the assistant role.|\n\nA multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](/docs/guides/conversation-state).\n\nYou could think about `developer` and `user` messages like a function and its arguments in a programming language.\n\n*   `developer` messages provide the system's rules and business logic, like a function definition.\n*   `user` messages provide inputs and configuration to which the `developer` message instructions are applied, like arguments to a function.",
    "source_file": "text generation.md",
    "chunk_index": 6,
    "char_count": 1366,
    "vector_index": 16,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_OfIwzhumhUuyaqibq5ZO2GLa",
    "text": "Reusable prompts\n----------------\n\nIn the OpenAI dashboard, you can develop reusable [prompts](/chat/edit) that you can use in API requests, rather than specifying the content of prompts in code. This way, you can more easily build and evaluate your prompts, and deploy improved versions of your prompts without changing your integration code.\n\nHere's how it works:\n\n1.  **Create a reusable prompt** in the [dashboard](/chat/edit) with placeholders like `{{customer_name}}`.\n2.  **Use the prompt** in your API request with the `prompt` parameter. The prompt parameter object has three properties you can configure:\n    *   `id` — Unique identifier of your prompt, found in the dashboard\n    *   `version` — A specific version of your prompt (defaults to the \"current\" version as specified in the dashboard)\n    *   `variables` — A map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input message types like `input_image` or `input_file`. [See the full API reference](/docs/api-reference/responses/create).",
    "source_file": "text generation.md",
    "chunk_index": 7,
    "char_count": 1080,
    "vector_index": 17,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_cU8QLUvCVCnnSBpH9kRmqeoV",
    "text": "String variables\n\nGenerate text with a prompt template\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    prompt={\n        \"id\": \"pmpt_abc123\",\n        \"version\": \"2\",\n        \"variables\": {\n            \"customer_name\": \"Jane Doe\",\n            \"product\": \"40oz juice box\"\n        }\n    }\n)\n\nprint(response.output_text)\n```",
    "source_file": "text generation.md",
    "chunk_index": 8,
    "char_count": 387,
    "vector_index": 18,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_mq4Ar6NcGJIdZjLiPehSTKG5",
    "text": "Variables with file input\n\nPrompt template with file input variable\n\n\n```python\nimport openai, pathlib\n\nclient = openai.OpenAI()\n\n# Upload a PDF we will reference in the variables\nfile = client.files.create(\n    file=open(\"draconomicon.pdf\", \"rb\"),\n    purpose=\"user_data\",\n)\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    prompt={\n        \"id\": \"pmpt_abc123\",\n        \"variables\": {\n            \"topic\": \"Dragons\",\n            \"reference_pdf\": {\n                \"type\": \"input_file\",\n                \"file_id\": file.id,\n            },\n        },\n    },\n)\n\nprint(response.output_text)\n```",
    "source_file": "text generation.md",
    "chunk_index": 9,
    "char_count": 601,
    "vector_index": 19,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_2RQnDvl9VxdEsPCP0v5L40RY",
    "text": "Next steps\n----------\n\nNow that you known the basics of text inputs and outputs, you might want to check out one of these resources next.\n\n[\n\nBuild a prompt in the Playground\n\nUse the Playground to develop and iterate on prompts.\n\n](/chat/edit)[\n\nGenerate JSON data with Structured Outputs\n\nEnsure JSON data emitted from a model conforms to a JSON schema.\n\n](/docs/guides/structured-outputs)[\n\nFull API reference\n\nCheck out all the options for text generation in the API reference.\n\n](/docs/api-reference/responses)\n\nWas this page useful?",
    "source_file": "text generation.md",
    "chunk_index": 10,
    "char_count": 538,
    "vector_index": 20,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_Xia0qrl5zipobCNEAc0jvsSc",
    "text": "Function calling\n================\n\nGive models access to new functionality and data they can use to follow instructions and respond to prompts.\n\n**Function calling** (also known as **tool calling**) provides a powerful and flexible way for OpenAI models to interface with external systems and access data outside their training data. This guide shows how you can connect a model to data and actions provided by your application. We'll show how to use function tools (defined by a JSON schema) and custom tools which work with free form text inputs and outputs.",
    "source_file": "function calling.md",
    "chunk_index": 0,
    "char_count": 560,
    "vector_index": 21,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_o4Rjfm8bjbhI6HdjTv6OumPP",
    "text": "How it works\n------------\n\nLet's begin by understanding a few key terms about tool calling. After we have a shared vocabulary for tool calling, we'll show you how it's done with some practical examples.\n\nTools - functionality we give the model\n\nA **function** or **tool** refers in the abstract to a piece of functionality that we tell the model it has access to. As a model generates a response to a prompt, it may decide that it needs data or functionality provided by a tool to follow the prompt's instructions.\n\nYou could give the model access to tools that:\n\n*   Get today's weather for a location\n*   Access account details for a given user ID\n*   Issue refunds for a lost order\n\nOr anything else you'd like the model to be able to know or do as it responds to a prompt.\n\nWhen we make an API request to the model with a prompt, we can include a list of tools the model could consider using. For example, if we wanted the model to be able to answer questions about the current weather somewhere in the world, we might give it access to a `get_weather` tool that takes `location` as an argument.\n\nTool calls - requests from the model to use tools\n\nA **function call** or **tool call** refers to a special kind of response we can get from the model if it examines a prompt, and then determines that in order to follow the instructions in the prompt, it needs to call one of the tools we made available to it.\n\nIf the model receives a prompt like \"what is the weather in Paris?\" in an API request, it could respond to that prompt with a tool call for the `get_weather` tool, with `Paris` as the `location` argument.\n\nTool call outputs - output we generate for the model\n\nA **function call output** or **tool call output** refers to the response a tool generates using the input from a model's tool call. The tool call output can either be structured JSON or plain text, and it should contain a reference to a specific model tool call (referenced by `call_id` in the examples to come). To complete our weather example:\n\n*   The model has access to a `get_weather` **tool** that takes `location` as an argument.\n*   In response to a prompt like \"what's the weather in Paris?\" the model returns a **tool call** that contains a `location` argument with a value of `Paris`\n*   The **tool call output** might return a JSON object (e.g., `{\"temperature\": \"25\", \"unit\": \"C\"}`, indicating a current temperature of 25 degrees), [Image contents](/docs/guides/images), or [File contents](/docs/guides/pdf-files).\n\nWe then send all of the tool definition, the original prompt, the model's tool call, and the tool call output back to the model to finally receive a text response like:\n\n```text\nThe weather in Paris today is 25C.\n```",
    "source_file": "function calling.md",
    "chunk_index": 1,
    "char_count": 2720,
    "vector_index": 22,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_xqb6ZTsDBWaIAC0iEqAI3Hb8",
    "text": "Functions versus tools\n\n*   A function is a specific kind of tool, defined by a JSON schema. A function definition allows the model to pass data to your application, where your code can access data or take actions suggested by the model.\n*   In addition to function tools, there are custom tools (described in this guide) that work with free text inputs and outputs.\n*   There are also [built-in tools](/docs/guides/tools) that are part of the OpenAI platform. These tools enable the model to [search the web](/docs/guides/tools-web-search), [execute code](/docs/guides/tools-code-interpreter), access the functionality of an [MCP server](/docs/guides/tools-remote-mcp), and more.\n\n### The tool calling flow\n\nTool calling is a multi-step conversation between your application and a model via the OpenAI API. The tool calling flow has five high level steps:\n\n1.  Make a request to the model with tools it could call\n2.  Receive a tool call from the model\n3.  Execute code on the application side with input from the tool call\n4.  Make a second request to the model with the tool output\n5.  Receive a final response from the model (or more tool calls)\n\n![Function Calling Diagram Steps](https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png)",
    "source_file": "function calling.md",
    "chunk_index": 2,
    "char_count": 1259,
    "vector_index": 23,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_qOEDIpoVOGXXlIgUSUvEQqQ1",
    "text": "Function tool example\n---------------------\n\nLet's look at an end-to-end tool calling flow for a `get_horoscope` function that gets a daily horoscope for an astrological sign.\n\nComplete tool calling example\n\n```python\nfrom openai import OpenAI\nimport json\n\nclient = OpenAI()\n\n# 1. Define a list of callable tools for the model\ntools = [\n    {\n        \"type\": \"function\",\n        \"name\": \"get_horoscope\",\n        \"description\": \"Get today's horoscope for an astrological sign.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sign\": {\n                    \"type\": \"string\",\n                    \"description\": \"An astrological sign like Taurus or Aquarius\",\n                },\n            },\n            \"required\": [\"sign\"],\n        },\n    },\n]\n\ndef get_horoscope(sign):\n    return f\"{sign}: Next Tuesday you will befriend a baby otter.\"\n\n# Create a running input list we will add to over time\ninput_list = [\n    {\"role\": \"user\", \"content\": \"What is my horoscope? I am an Aquarius.\"}\n]\n\n# 2. Prompt the model with tools defined\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    tools=tools,\n    input=input_list,\n)\n\n# Save function call outputs for subsequent requests\ninput_list += response.output\n\nfor item in response.output:\n    if item.type == \"function_call\":\n        if item.name == \"get_horoscope\":\n            # 3. Execute the function logic for get_horoscope\n            horoscope = get_horoscope(json.loads(item.arguments))\n            \n            # 4. Provide function call results to the model\n            input_list.append({\n                \"type\": \"function_call_output\",\n                \"call_id\": item.call_id,\n                \"output\": json.dumps({\n                  \"horoscope\": horoscope\n                })\n            })\n\nprint(\"Final input:\")\nprint(input_list)\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    instructions=\"Respond only with a horoscope generated by a tool.\",\n    tools=tools,\n    input=input_list,\n)\n\n# 5. The model should be able to give a response!\nprint(\"Final output:\")\nprint(response.model_dump_json(indent=2))\nprint(\"\\n\" + response.output_text)\n```\n\n\nNote that for reasoning models like GPT-5 or o4-mini, any reasoning items returned in model responses with tool calls must also be passed back with tool call outputs.",
    "source_file": "function calling.md",
    "chunk_index": 3,
    "char_count": 2334,
    "vector_index": 24,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_D562rX84m2fET1E0bW2EGPbq",
    "text": "Defining functions\n------------------\n\nFunctions can be set in the `tools` parameter of each API request. A function is defined by its schema, which informs the model what it does and what input arguments it expects. A function definition has the following properties:\n\n|Field|Description|\n|---|---|\n|type|This should always be function|\n|name|The function's name (e.g. get_weather)|\n|description|Details on when and how to use the function|\n|parameters|JSON schema defining the function's input arguments|\n|strict|Whether to enforce strict mode for the function call|\n\nHere is an example function definition for a `get_weather` function\n\n```json\n{\n    \"type\": \"function\",\n    \"name\": \"get_weather\",\n    \"description\": \"Retrieves current weather for the given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City and country e.g. Bogotá, Colombia\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"description\": \"Units the temperature will be returned in.\"\n            }\n        },\n        \"required\": [\"location\", \"units\"],\n        \"additionalProperties\": false\n    },\n    \"strict\": true\n}\n```\n\nBecause the `parameters` are defined by a [JSON schema](https://json-schema.org/), you can leverage many of its rich features like property types, enums, descriptions, nested objects, and, recursive objects.",
    "source_file": "function calling.md",
    "chunk_index": 4,
    "char_count": 1519,
    "vector_index": 25,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_8aibAmam4YSnSHeN5RpaKop0",
    "text": "### Best practices for defining functions\n\n1.  **Write clear and detailed function names, parameter descriptions, and instructions.**\n    \n    *   **Explicitly describe the purpose of the function and each parameter** (and its format), and what the output represents.\n    *   **Use the system prompt to describe when (and when not) to use each function.** Generally, tell the model _exactly_ what to do.\n    *   **Include examples and edge cases**, especially to rectify any recurring failures. (**Note:** Adding examples may hurt performance for [reasoning models](/docs/guides/reasoning).)\n2.  **Apply software engineering best practices.**\n    \n    *   **Make the functions obvious and intuitive**. ([principle of least surprise](https://en.wikipedia.org/wiki/Principle_of_least_astonishment))\n    *   **Use enums** and object structure to make invalid states unrepresentable. (e.g. `toggle_light(on: bool, off: bool)` allows for invalid calls)\n    *   **Pass the intern test.** Can an intern/human correctly use the function given nothing but what you gave the model? (If not, what questions do they ask you? Add the answers to the prompt.)\n3.  **Offload the burden from the model and use code where possible.**\n    \n    *   **Don't make the model fill arguments you already know.** For example, if you already have an `order_id` based on a previous menu, don't have an `order_id` param – instead, have no params `submit_refund()` and pass the `order_id` with code.\n    *   **Combine functions that are always called in sequence.** For example, if you always call `mark_location()` after `query_location()`, just move the marking logic into the query function call.\n4.  **Keep the number of functions small for higher accuracy.**\n    \n    *   **Evaluate your performance** with different numbers of functions.\n    *   **Aim for fewer than 20 functions** at any one time, though this is just a soft suggestion.\n5.  **Leverage OpenAI resources.**\n    \n    *   **Generate and iterate on function schemas** in the [Playground](/playground).\n    *   **Consider [fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) to increase function calling accuracy** for large numbers of functions or difficult tasks. ([cookbook](https://cookbook.openai.com/examples/fine_tuning_for_function_calling))",
    "source_file": "function calling.md",
    "chunk_index": 5,
    "char_count": 2301,
    "vector_index": 26,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_MBdTi8aPtCtOS3jtsEeOO9Xi",
    "text": "### Token Usage\n\nUnder the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If you run into token limits, we suggest limiting the number of functions or the length of the descriptions you provide for function parameters.\n\nIt is also possible to use [fine-tuning](/docs/guides/fine-tuning#fine-tuning-examples) to reduce the number of tokens used if you have many functions defined in your tools specification.",
    "source_file": "function calling.md",
    "chunk_index": 6,
    "char_count": 549,
    "vector_index": 27,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_CQOW43dtzFpthw7r1K6Yiyiw",
    "text": "Handling function calls\n-----------------------\n\nWhen the model calls a function, you must execute it and return the result. Since model responses can include zero, one, or multiple calls, it is best practice to assume there are several.\n\nThe response `output` array contains an entry with the `type` having a value of `function_call`. Each entry with a `call_id` (used later to submit the function result), `name`, and JSON-encoded `arguments`.\n\nSample response with multiple function calls\n\n```json\n[\n    {\n        \"id\": \"fc_12345xyz\",\n        \"call_id\": \"call_12345xyz\",\n        \"type\": \"function_call\",\n        \"name\": \"get_weather\",\n        \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\"\n    },\n    {\n        \"id\": \"fc_67890abc\",\n        \"call_id\": \"call_67890abc\",\n        \"type\": \"function_call\",\n        \"name\": \"get_weather\",\n        \"arguments\": \"{\\\"location\\\":\\\"Bogotá, Colombia\\\"}\"\n    },\n    {\n        \"id\": \"fc_99999def\",\n        \"call_id\": \"call_99999def\",\n        \"type\": \"function_call\",\n        \"name\": \"send_email\",\n        \"arguments\": \"{\\\"to\\\":\\\"bob@email.com\\\",\\\"body\\\":\\\"Hi bob\\\"}\"\n    }\n]\n```\n\nExecute function calls and append results\n\n```python\nfor tool_call in response.output:\n    if tool_call.type != \"function_call\":\n        continue\n\n    name = tool_call.name\n    args = json.loads(tool_call.arguments)\n\n    result = call_function(name, args)\n    input_messages.append({\n        \"type\": \"function_call_output\",\n        \"call_id\": tool_call.call_id,\n        \"output\": str(result)\n    })\n```\n\n\nIn the example above, we have a hypothetical `call_function` to route each call. Here’s a possible implementation:\n\nExecute function calls and append results\n\n```python\ndef call_function(name, args):\n    if name == \"get_weather\":\n        return get_weather(**args)\n    if name == \"send_email\":\n        return send_email(**args)\n```\n\n\n### Formatting results\n\nA result must be a string, but the format is up to you (JSON, error codes, plain text, etc.). The model will interpret that string as needed.\n\nIf your function has no return value (e.g. `send_email`), simply return a string to indicate success or failure. (e.g. `\"success\"`)\n\n### Incorporating results into response\n\nAfter appending the results to your `input`, you can send them back to the model to get a final response.\n\nSend results back to model\n\n```python\nresponse = client.responses.create(\n    model=\"gpt-4.1\",\n    input=input_messages,\n    tools=tools,\n)\n```\n\n\nFinal response\n\n```json\n\"It's about 15°C in Paris, 18°C in Bogotá, and I've sent that email to Bob.\"\n```",
    "source_file": "function calling.md",
    "chunk_index": 7,
    "char_count": 2556,
    "vector_index": 28,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_OxFTXIsKWjGjZepi2rkmXLys",
    "text": "Additional configurations\n-------------------------\n\n### Tool choice\n\nBy default the model will determine when and how many tools to use. You can force specific behavior with the `tool_choice` parameter.\n\n1.  **Auto:** (_Default_) Call zero, one, or multiple functions. `tool_choice: \"auto\"`\n2.  **Required:** Call one or more functions. `tool_choice: \"required\"`\n3.  **Forced Function:** Call exactly one specific function. `tool_choice: {\"type\": \"function\", \"name\": \"get_weather\"}`\n4.  **Allowed tools:** Restrict the tool calls the model can make to a subset of the tools available to the model.\n\n**When to use allowed\\_tools**\n\nYou might want to configure an `allowed_tools` list in case you want to make only a subset of tools available across model requests, but not modify the list of tools you pass in, so you can maximize savings from [prompt caching](/docs/guides/prompt-caching).\n\n```json\n\"tool_choice\": {\n    \"type\": \"allowed_tools\",\n    \"mode\": \"auto\",\n    \"tools\": [\n        { \"type\": \"function\", \"name\": \"get_weather\" },\n        { \"type\": \"function\", \"name\": \"search_docs\" }\n    ]\n  }\n}\n```\n\nYou can also set `tool_choice` to `\"none\"` to imitate the behavior of passing no functions.\n\n### Parallel function calling\n\nParallel function calling is not possible when using [built-in tools](/docs/guides/tools).\n\nThe model may choose to call multiple functions in a single turn. You can prevent this by setting `parallel_tool_calls` to `false`, which ensures exactly zero or one tool is called.\n\n**Note:** Currently, if you are using a fine tuned model and the model calls multiple functions in one turn then [strict mode](/docs/guides/function-calling#strict-mode) will be disabled for those calls.\n\n**Note for `gpt-4.1-nano-2025-04-14`:** This snapshot of `gpt-4.1-nano` can sometimes include multiple tools calls for the same tool if parallel tool calls are enabled. It is recommended to disable this feature when using this nano snapshot.\n\n### Strict mode\n\nSetting `strict` to `true` will ensure function calls reliably adhere to the function schema, instead of being best effort. We recommend always enabling strict mode.\n\nUnder the hood, strict mode works by leveraging our [structured outputs](/docs/guides/structured-outputs) feature and therefore introduces a couple requirements:\n\n1.  `additionalProperties` must be set to `false` for each object in the `parameters`.\n2.  All fields in `properties` must be marked as `required`.\n\nYou can denote optional fields by adding `null` as a `type` option (see example below).\n\nStrict mode enabled\n\n```json\n{\n    \"type\": \"function\",\n    \"name\": \"get_weather\",\n    \"description\": \"Retrieves current weather for the given location.\",\n    \"strict\": true,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City and country e.g. Bogotá, Colombia\"\n            },\n            \"units\": {\n                \"type\": [\"string\", \"null\"],\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"description\": \"Units the temperature will be returned in.\"\n            }\n        },\n        \"required\": [\"location\", \"units\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\nStrict mode disabled\n\n```json\n{\n    \"type\": \"function\",\n    \"name\": \"get_weather\",\n    \"description\": \"Retrieves current weather for the given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City and country e.g. Bogotá, Colombia\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"description\": \"Units the temperature will be returned in.\"\n            }\n        },\n        \"required\": [\"location\"],\n    }\n}\n```\n\nAll schemas generated in the [playground](/playground) have strict mode enabled.\n\nWhile we recommend you enable strict mode, it has a few limitations:\n\n1.  Some features of JSON schema are not supported. (See [supported schemas](/docs/guides/structured-outputs?context=with_parse#supported-schemas).)\n\nSpecifically for fine tuned models:\n\n1.  Schemas undergo additional processing on the first request (and are then cached). If your schemas vary from request to request, this may result in higher latencies.\n2.  Schemas are cached for performance, and are not eligible for [zero data retention](/docs/models#how-we-use-your-data).",
    "source_file": "function calling.md",
    "chunk_index": 8,
    "char_count": 4501,
    "vector_index": 29,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_9DTlOG7LoeFvGfQwP8bOg2Vu",
    "text": "Streaming\n---------\n\nStreaming can be used to surface progress by showing which function is called as the model fills its arguments, and even displaying the arguments in real time.\n\nStreaming function calls is very similar to streaming regular responses: you set `stream` to `true` and get different `event` objects.\n\nStreaming function calls\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntools = [{\n    \"type\": \"function\",\n    \"name\": \"get_weather\",\n    \"description\": \"Get current temperature for a given location.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City and country e.g. Bogotá, Colombia\"\n            }\n        },\n        \"required\": [\n            \"location\"\n        ],\n        \"additionalProperties\": False\n    }\n}]\n\nstream = client.responses.create(\n    model=\"gpt-4.1\",\n    input=[{\"role\": \"user\", \"content\": \"What's the weather like in Paris today?\"}],\n    tools=tools,\n    stream=True\n)\n\nfor event in stream:\n    print(event)\n```\n\n\nOutput events\n\n```json\n{\"type\":\"response.output_item.added\",\"response_id\":\"resp_1234xyz\",\"output_index\":0,\"item\":{\"type\":\"function_call\",\"id\":\"fc_1234xyz\",\"call_id\":\"call_1234xyz\",\"name\":\"get_weather\",\"arguments\":\"\"}}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\"{\\\"\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\"location\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\"\\\":\\\"\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\"Paris\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\",\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\" France\"}\n{\"type\":\"response.function_call_arguments.delta\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"delta\":\"\\\"}\"}\n{\"type\":\"response.function_call_arguments.done\",\"response_id\":\"resp_1234xyz\",\"item_id\":\"fc_1234xyz\",\"output_index\":0,\"arguments\":\"{\\\"location\\\":\\\"Paris, France\\\"}\"}\n{\"type\":\"response.output_item.done\",\"response_id\":\"resp_1234xyz\",\"output_index\":0,\"item\":{\"type\":\"function_call\",\"id\":\"fc_1234xyz\",\"call_id\":\"call_1234xyz\",\"name\":\"get_weather\",\"arguments\":\"{\\\"location\\\":\\\"Paris, France\\\"}\"}}\n```\n\nInstead of aggregating chunks into a single `content` string, however, you're aggregating chunks into an encoded `arguments` JSON object.\n\nWhen the model calls one or more functions an event of type `response.output_item.added` will be emitted for each function call that contains the following fields:\n\n|Field|Description|\n|---|---|\n|response_id|The id of the response that the function call belongs to|\n|output_index|The index of the output item in the response. This represents the individual function calls in the response.|\n|item|The in-progress function call item that includes a name, arguments and id field|\n\nAfterwards you will receive a series of events of type `response.function_call_arguments.delta` which will contain the `delta` of the `arguments` field. These events contain the following fields:\n\n|Field|Description|\n|---|---|\n|response_id|The id of the response that the function call belongs to|\n|item_id|The id of the function call item that the delta belongs to|\n|output_index|The index of the output item in the response. This represents the individual function calls in the response.|\n|delta|The delta of the arguments field.|\n\nBelow is a code snippet demonstrating how to aggregate the `delta`s into a final `tool_call` object.\n\nAccumulating tool\\_call deltas\n\n```python\nfinal_tool_calls = {}\n\nfor event in stream:\n    if event.type === 'response.output_item.added':\n        final_tool_calls[event.output_index] = event.item;\n    elif event.type === 'response.function_call_arguments.delta':\n        index = event.output_index\n\n        if final_tool_calls[index]:\n            final_tool_calls[index].arguments += event.delta\n```\n\n\nAccumulated final\\_tool\\_calls\\[0\\]\n\n```json\n{\n    \"type\": \"function_call\",\n    \"id\": \"fc_1234xyz\",\n    \"call_id\": \"call_2345abc\",\n    \"name\": \"get_weather\",\n    \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\"\n}\n```\n\nWhen the model has finished calling the functions an event of type `response.function_call_arguments.done` will be emitted. This event contains the entire function call including the following fields:\n\n|Field|Description|\n|---|---|\n|response_id|The id of the response that the function call belongs to|\n|output_index|The index of the output item in the response. This represents the individual function calls in the response.|\n|item|The function call item that includes a name, arguments and id field.|",
    "source_file": "function calling.md",
    "chunk_index": 9,
    "char_count": 5010,
    "vector_index": 30,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_sxkSGvjMBzW6CUCEaabaXvCq",
    "text": "Custom tools\n------------\n\nCustom tools work in much the same way as JSON schema-driven function tools. But rather than providing the model explicit instructions on what input your tool requires, the model can pass an arbitrary string back to your tool as input. This is useful to avoid unnecessarily wrapping a response in JSON, or to apply a custom grammar to the response (more on this below).\n\nThe following code sample shows creating a custom tool that expects to receive a string of text containing Python code as a response.\n\nCustom tool calling example\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Use the code_exec tool to print hello world to the console.\",\n    tools=[\n        {\n            \"type\": \"custom\",\n            \"name\": \"code_exec\",\n            \"description\": \"Executes arbitrary Python code.\",\n        }\n    ]\n)\nprint(response.output)\n```\n\n\nJust as before, the `output` array will contain a tool call generated by the model. Except this time, the tool call input is given as plain text.\n\n```json\n[\n    {\n        \"id\": \"rs_6890e972fa7c819ca8bc561526b989170694874912ae0ea6\",\n        \"type\": \"reasoning\",\n        \"content\": [],\n        \"summary\": []\n    },\n    {\n        \"id\": \"ctc_6890e975e86c819c9338825b3e1994810694874912ae0ea6\",\n        \"type\": \"custom_tool_call\",\n        \"status\": \"completed\",\n        \"call_id\": \"call_aGiFQkRWSWAIsMQ19fKqxUgb\",\n        \"input\": \"print(\\\"hello world\\\")\",\n        \"name\": \"code_exec\"\n    }\n]\n```",
    "source_file": "function calling.md",
    "chunk_index": 10,
    "char_count": 1530,
    "vector_index": 31,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_go2tnUCuFBDBWXgMZbSoVccA",
    "text": "Context-free grammars\n---------------------\n\nA [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar) (CFG) is a set of rules that define how to produce valid text in a given format. For custom tools, you can provide a CFG that will constrain the model's text input for a custom tool.\n\nYou can provide a custom CFG using the `grammar` parameter when configuring a custom tool. Currently, we support two CFG syntaxes when defining grammars: `lark` and `regex`.",
    "source_file": "function calling.md",
    "chunk_index": 11,
    "char_count": 479,
    "vector_index": 32,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_eEibjYgCio8Hb1kbW3WmLy6y",
    "text": "Lark CFG\n--------\n\nLark context free grammar example\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ngrammar = \"\"\"\nstart: expr\nexpr: term (SP ADD SP term)* -> add\n| term\nterm: factor (SP MUL SP factor)* -> mul\n| factor\nfactor: INT\nSP: \" \"\nADD: \"+\"\nMUL: \"*\"\n%import common.INT\n\"\"\"\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Use the math_exp tool to add four plus four.\",\n    tools=[\n        {\n            \"type\": \"custom\",\n            \"name\": \"math_exp\",\n            \"description\": \"Creates valid mathematical expressions\",\n            \"format\": {\n                \"type\": \"grammar\",\n                \"syntax\": \"lark\",\n                \"definition\": grammar,\n            },\n        }\n    ]\n)\nprint(response.output)\n```\n\n\nThe output from the tool should then conform to the Lark CFG that you defined:\n\n```json\n[\n    {\n        \"id\": \"rs_6890ed2b6374819dbbff5353e6664ef103f4db9848be4829\",\n        \"type\": \"reasoning\",\n        \"content\": [],\n        \"summary\": []\n    },\n    {\n        \"id\": \"ctc_6890ed2f32e8819daa62bef772b8c15503f4db9848be4829\",\n        \"type\": \"custom_tool_call\",\n        \"status\": \"completed\",\n        \"call_id\": \"call_pmlLjmvG33KJdyVdC4MVdk5N\",\n        \"input\": \"4 + 4\",\n        \"name\": \"math_exp\"\n    }\n]\n```\n\nGrammars are specified using a variation of [Lark](https://lark-parser.readthedocs.io/en/stable/index.html). Model sampling is constrained using [LLGuidance](https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md). Some features of Lark are not supported:\n\n*   Lookarounds in lexer regexes\n*   Lazy modifiers (`*?`, `+?`, `??`) in lexer regexes\n*   Priorities of terminals\n*   Templates\n*   Imports (other than built-in `%import` common)\n*   `%declare`s\n\nWe recommend using the [Lark IDE](https://www.lark-parser.org/ide/) to experiment with custom grammars.\n\n### Keep grammars simple\n\nTry to make your grammar as simple as possible. The OpenAI API may return an error if the grammar is too complex, so you should ensure that your desired grammar is compatible before using it in the API.\n\nLark grammars can be tricky to perfect. While simple grammars perform most reliably, complex grammars often require iteration on the grammar definition itself, the prompt, and the tool description to ensure that the model does not go out of distribution.\n\n### Correct versus incorrect patterns\n\nCorrect (single, bounded terminal):\n\n```text\nstart: SENTENCE\nSENTENCE: /[A-Za-z, ]*(the hero|a dragon|an old man|the princess)[A-Za-z, ]*(fought|saved|found|lost)[A-Za-z, ]*(a treasure|the kingdom|a secret|his way)[A-Za-z, ]*\\./\n```\n\nDo NOT do this (splitting across rules/terminals). This attempts to let rules partition free text between terminals. The lexer will greedily match the free-text pieces and you'll lose control:\n\n```text\nstart: sentence\nsentence: /[A-Za-z, ]+/ subject /[A-Za-z, ]+/ verb /[A-Za-z, ]+/ object /[A-Za-z, ]+/\n```\n\nLowercase rules don't influence how terminals are cut from the input—only terminal definitions do. When you need “free text between anchors,” make it one giant regex terminal so the lexer matches it exactly once with the structure you intend.\n\n### Terminals versus rules\n\nLark uses terminals for lexer tokens (by convention, `UPPERCASE`) and rules for parser productions (by convention, `lowercase`). The most practical way to stay within the supported subset and avoid surprises is to keep your grammar simple and explicit, and to use terminals and rules with a clear separation of concerns.\n\nThe regex syntax used by terminals is the [Rust regex crate syntax](https://docs.rs/regex/latest/regex/#syntax), not Python's `re` [module](https://docs.python.org/3/library/re.html).\n\n### Key ideas and best practices\n\n**Lexer runs before the parser**\n\nTerminals are matched by the lexer (greedily / longest match wins) before any CFG rule logic is applied. If you try to \"shape\" a terminal by splitting it across several rules, the lexer cannot be guided by those rules—only by terminal regexes.\n\n**Prefer one terminal when you're carving text out of freeform spans**\n\nIf you need to recognize a pattern embedded in arbitrary text (e.g., natural language with “anything” between anchors), express that as a single terminal. Do not try to interleave free‑text terminals with parser rules; the greedy lexer will not respect your intended boundaries and it is highly likely the model will go out of distribution.\n\n**Use rules to compose discrete tokens**\n\nRules are ideal when you're combining clearly delimited terminals (numbers, keywords, punctuation) into larger structures. They're not the right tool for constraining \"the stuff in between\" two terminals.\n\n**Keep terminals simple, bounded, and self-contained**\n\nFavor explicit character classes and bounded quantifiers (`{0,10}`, not unbounded `*` everywhere). If you need \"any text up to a period\", prefer something like `/[^.\\n]{0,10}*\\./` rather than `/.+\\./` to avoid runaway growth.\n\n**Use rules to combine tokens, not to steer regex internals**\n\nGood rule usage example:\n\n```text\nstart: expr\nNUMBER: /[0-9]+/\nPLUS: \"+\"\nMINUS: \"-\"\nexpr: term ((\"+\"|\"-\") term)*\nterm: NUMBER\n```\n\n**Treat whitespace explicitly**\n\nDon't rely on open-ended `%ignore` directives. Using unbounded ignore directives may cause the grammar to be too complex and/or may cause the model to go out of distribution. Prefer threading explicit terminals wherever whitespace is allowed.\n\n### Troubleshooting\n\n*   If the API rejects the grammar because it is too complex, simplify the rules and terminals and remove unbounded `%ignore`s.\n*   If custom tools are called with unexpected tokens, confirm terminals aren’t overlapping; check greedy lexer.\n*   When the model drifts \"out‑of‑distribution\" (shows up as the model producing excessively long or repetitive outputs, it is syntactically valid but is semantically wrong):\n    *   Tighten the grammar.\n    *   Iterate on the prompt (add few-shot examples) and tool description (explain the grammar and instruct the model to reason and conform to it).\n    *   Experiment with a higher reasoning effort (e.g, bump from medium to high).",
    "source_file": "function calling.md",
    "chunk_index": 12,
    "char_count": 6102,
    "vector_index": 33,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_yGIekxypqOF8d0Ph2hqRnfRP",
    "text": "Regex CFG\n---------\n\nRegex context free grammar example\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ngrammar = r\"^(?P<month>January|February|March|April|May|June|July|August|September|October|November|December)\\s+(?P<day>\\d{1,2})(?:st|nd|rd|th)?\\s+(?P<year>\\d{4})\\s+at\\s+(?P<hour>0?[1-9]|1[0-2])(?P<ampm>AM|PM)$\"\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Use the timestamp tool to save a timestamp for August 7th 2025 at 10AM.\",\n    tools=[\n        {\n            \"type\": \"custom\",\n            \"name\": \"timestamp\",\n            \"description\": \"Saves a timestamp in date + time in 24-hr format.\",\n            \"format\": {\n                \"type\": \"grammar\",\n                \"syntax\": \"regex\",\n                \"definition\": grammar,\n            },\n        }\n    ]\n)\nprint(response.output)\n```\n\n\nThe output from the tool should then conform to the Regex CFG that you defined:\n\n```json\n[\n    {\n        \"id\": \"rs_6894f7a3dd4c81a1823a723a00bfa8710d7962f622d1c260\",\n        \"type\": \"reasoning\",\n        \"content\": [],\n        \"summary\": []\n    },\n    {\n        \"id\": \"ctc_6894f7ad7fb881a1bffa1f377393b1a40d7962f622d1c260\",\n        \"type\": \"custom_tool_call\",\n        \"status\": \"completed\",\n        \"call_id\": \"call_8m4XCnYvEmFlzHgDHbaOCFlK\",\n        \"input\": \"August 7th 2025 at 10AM\",\n        \"name\": \"timestamp\"\n    }\n]\n```\n\nAs with the Lark syntax, regexes use the [Rust regex crate syntax](https://docs.rs/regex/latest/regex/#syntax), not Python's `re` [module](https://docs.python.org/3/library/re.html).\n\nSome features of Regex are not supported:\n\n*   Lookarounds\n*   Lazy modifiers (`*?`, `+?`, `??`)\n\n### Key ideas and best practices\n\n**Pattern must be on one line**\n\nIf you need to match a newline in the input, use the escaped sequence `\\n`. Do not use verbose/extended mode, which allows patterns to span multiple lines.\n\n**Provide the regex as a plain pattern string**\n\nDon't enclose the pattern in `//`.\n\nWas this page useful?",
    "source_file": "function calling.md",
    "chunk_index": 13,
    "char_count": 1965,
    "vector_index": 34,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_ikQLWd48AoatJzHgPWz0UyhZ",
    "text": "Using GPT-5\n===========\n\nLearn best practices, features, and migration guidance for GPT-5.\n\nGPT-5 is our most intelligent model yet, trained to be especially proficient in:\n\n*   Code generation, bug fixing, and refactoring\n*   Instruction following\n*   Long context and tool calling\n\nThis guide covers key features of the GPT-5 model family and how to get the most out of GPT-5.\n\n### Explore coding examples\n\nClick through a few demo applications generated entirely with a single GPT-5 prompt, without writing any code by hand.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 0,
    "char_count": 527,
    "vector_index": 35,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_QM119Zb45233gyO2suIY9bsX",
    "text": "Quickstart\n----------\n\nFaster responses\n\nBy default, GPT-5 produces a medium length chain of thought before responding to a prompt. For faster, lower-latency responses, use low reasoning effort and low text verbosity.  \n  \nThis behavior will more closely (but not exactly!) match non-reasoning models like [GPT-4.1](/docs/models/gpt-4.1). We expect GPT-5 to produce more intelligent responses than GPT-4.1, but when speed and maximum context length are paramount, you might consider using GPT-4.1 instead.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 1,
    "char_count": 505,
    "vector_index": 36,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_aQgBzKxI6LUIhQdcqMxC6aRk",
    "text": "Fast, low latency response options\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresult = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Write a haiku about code.\",\n    reasoning={ \"effort\": \"low\" },\n    text={ \"verbosity\": \"low\" },\n)\n\nprint(result.output_text)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 2,
    "char_count": 284,
    "vector_index": 37,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_faOWtXi6J48qzCgUcm0s8jlP",
    "text": "Coding and agentic tasks\n\nGPT-5 is great at reasoning through complex tasks. **For complex tasks like coding and multi-step planning, use high reasoning effort.**  \n  \nUse these configurations when replacing tasks you might have used o3 to tackle. We expect GPT-5 to produce better results than o3 and o4-mini under most circumstances.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 3,
    "char_count": 335,
    "vector_index": 38,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_oZ25ZHZTxgaUXeLDVfj2fHrQ",
    "text": "Slower, high reasoning tasks\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresult = client.responses.create(\n    model=\"gpt-5\",\n    input=\"Find the null pointer exception: ...your code here...\",\n    reasoning={ \"effort\": \"high\" },\n)\n\nprint(result.output_text)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 4,
    "char_count": 274,
    "vector_index": 39,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_PkoDcC2TWto4dvM6PnrSk4zR",
    "text": "Meet the models\n---------------\n\nThere are three models in the GPT-5 series. In general, `gpt-5` is best for your most complex tasks that require broad world knowledge. The smaller mini and nano models trade off some general world knowledge for lower cost and lower latency. Small models will tend to perform better for more well defined tasks.\n\nTo help you pick the model that best fits your use case, consider these tradeoffs:\n\n|Variant|Best for|\n|---|---|\n|gpt-5|Complex reasoning, broad world knowledge, and code-heavy or multi-step agentic tasks|\n|gpt-5-mini|Cost-optimized reasoning and chat; balances speed, cost, and capability|\n|gpt-5-nano|High-throughput tasks, especially simple instruction-following or classification|",
    "source_file": "Using GPT 5.md",
    "chunk_index": 5,
    "char_count": 730,
    "vector_index": 40,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_Xhg3JxYXPP0fHCIq6uDUMTSn",
    "text": "### Model name reference\n\nThe GPT-5 [system card](https://openai.com/index/gpt-5-system-card/) uses different names than the API. Use this table to map between them:\n\n|System card name|API alias|\n|---|---|\n|gpt-5-thinking|gpt-5|\n|gpt-5-thinking-mini|gpt-5-mini|\n|gpt-5-thinking-nano|gpt-5-nano|\n|gpt-5-main|gpt-5-chat-latest|\n|gpt-5-main-mini|[not available via API]|",
    "source_file": "Using GPT 5.md",
    "chunk_index": 6,
    "char_count": 367,
    "vector_index": 41,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_8cEvDUeZjK8z9mJ6sDSae8zq",
    "text": "### New API features in GPT-5\n\nAlongside GPT-5, we're introducing a few new parameters and API features designed to give developers more control and flexibility: the ability to control verbosity, a minimal reasoning effort option, custom tools, and an allowed tools list.\n\nThis guide walks through some of the key features of the GPT-5 model family and how to get the most out of these models.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 7,
    "char_count": 393,
    "vector_index": 42,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_g5mXK1cI2c95Bq5ped3GN7Ts",
    "text": "Minimal reasoning effort\n------------------------\n\nThe `reasoning.effort` parameter controls how many reasoning tokens the model generates before producing a response. Earlier reasoning models like o3 supported only `low`, `medium`, and `high`: `low` favored speed and fewer tokens, while `high` favored more thorough reasoning.\n\nThe new `minimal` setting produces very few reasoning tokens for cases where you need the fastest possible time-to-first-token. We often see better performance when the model can produce a few tokens when needed versus none. The default is `medium`.\n\nThe `minimal` setting performs especially well in coding and instruction following scenarios, adhering closely to given directions. However, it may require prompting to act more proactively. To improve the model's reasoning quality, even at minimal effort, encourage it to “think” or outline its steps before answering.\n\nMinimal reasoning effort\n\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\",\n    reasoning={\n        \"effort\": \"minimal\"\n    }\n)\n\nprint(response)\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 8,
    "char_count": 1199,
    "vector_index": 43,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_mkIfBMsIbPXNyX6dVsGC04M0",
    "text": "### Verbosity\n\nVerbosity determines how many output tokens are generated. Lowering the number of tokens reduces overall latency. While the model's reasoning approach stays mostly the same, the model finds ways to answer more concisely—which can either improve or diminish answer quality, depending on your use case. Here are some scenarios for both ends of the verbosity spectrum:\n\n*   **High verbosity:** Use when you need the model to provide thorough explanations of documents or perform extensive code refactoring.\n*   **Low verbosity:** Best for situations where you want concise answers or simple code generation, such as SQL queries.\n\nModels before GPT-5 have used `medium` verbosity by default. With GPT-5, we make this option configurable as one of `high`, `medium`, or `low`.\n\nWhen generating code, `medium` and `high` verbosity levels yield longer, more structured code with inline explanations, while `low` verbosity produces shorter, more concise code with minimal commentary.\n\nControl verbosity\n\n\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n    text={\n        \"verbosity\": \"low\"\n    }\n)\n\nprint(response)\n```\n\nYou can still steer verbosity through prompting after setting it to `low` in the API. The verbosity parameter defines a general token range at the system prompt level, but the actual output is flexible to both developer and user prompts within that range.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 9,
    "char_count": 1541,
    "vector_index": 44,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_gSOYaooyYnf2UDC4A2VZj0H5",
    "text": "### Custom tools\n\nWith GPT-5, we're introducing a new capability called custom tools, which lets models send any raw text as tool call input but still constrain outputs if desired.\n\n[\n\nFunction calling guide\n\nLearn about custom tools in the function calling guide.\n\n](/docs/guides/function-calling)\n\n#### Freeform inputs\n\nDefine your tool with `type: custom` to enable models to send plaintext inputs directly to your tools, rather than being limited to structured JSON. The model can send any raw text—code, SQL queries, shell commands, configuration files, or long-form prose—directly to your tool.\n\n\n#### Constraining outputs\n\nGPT-5 supports context-free grammars (CFGs) for custom tools, letting you provide a Lark grammar to constrain outputs to a specific syntax or DSL. Attaching a CFG (e.g., a SQL or DSL grammar) ensures the assistant's text matches your grammar.\n\nThis enables precise, constrained tool calls or structured responses and lets you enforce strict syntactic or domain-specific formats directly in GPT-5's function calling, improving control and reliability for complex or constrained domains.\n\n#### Best practices for custom tools\n\n*   **Write concise, explicit tool descriptions**. The model chooses what to send based on your description; state clearly if you want it to always call the tool.\n*   **Validate outputs on the server side**. Freeform strings are powerful but require safeguards against injection or unsafe commands.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 10,
    "char_count": 1453,
    "vector_index": 45,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_aeuXARcTshC6unrLhZaTuo4u",
    "text": "### Allowed tools\n\nThe `allowed_tools` parameter under `tool_choice` lets you pass N tool definitions but restrict the model to only M (< N) of them. List your full toolkit in `tools`, and then use an `allowed_tools` block to name the subset and specify a mode—either `auto` (the model may pick any of those) or `required` (the model must invoke one).\n\n[\n\nFunction calling guide\n\nLearn about the allowed tools option in the function calling guide.\n\n](/docs/guides/function-calling)\n\nBy separating all possible tools from the subset that can be used _now_, you gain greater safety, predictability, and improved prompt caching. You also avoid brittle prompt engineering, such as hard-coded call order. GPT-5 dynamically invokes or requires specific functions mid-conversation while reducing the risk of unintended tool usage over long contexts.\n\n||Standard Tools|Allowed Tools|\n|---|---|---|\n|Model's universe|All tools listed under \"tools\": […]|Only the subset under \"tools\": […] in tool_choice|\n|Tool invocation|Model may or may not call any tool|Model restricted to (or required to call) chosen tools|\n|Purpose|Declare available capabilities|Constrain which capabilities are actually used|\n\n\nFor a more detailed overview of all of these new features, see the [accompanying cookbook](https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 11,
    "char_count": 1355,
    "vector_index": 46,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_65Oy7tw7Prh4bHFfjn72zXPw",
    "text": "### Preambles\n\nPreambles are brief, user-visible explanations that GPT-5 generates before invoking any tool or function, outlining its intent or plan (e.g., “why I'm calling this tool”). They appear after the chain-of-thought and before the actual tool call, providing transparency into the model's reasoning and enhancing debuggability, user confidence, and fine-grained steerability.\n\nBy letting GPT-5 “think out loud” before each tool call, preambles boost tool-calling accuracy (and overall task success) without bloating reasoning overhead. To enable preambles, add a system or developer instruction—for example: “Before you call a tool, explain why you are calling it.” GPT-5 prepends a concise rationale to each specified tool call. The model may also output multiple messages between tool calls, which can enhance the interaction experience—particularly for minimal reasoning or latency-sensitive use cases.\n\nFor more on using preambles, see the [GPT-5 prompting cookbook](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide#tool-preambles).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 12,
    "char_count": 1062,
    "vector_index": 47,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_QsEah2u7r7XZMp2hEGw0DujL",
    "text": "Migration guidance\n------------------\n\nGPT-5 is our best model yet, and it works best with the Responses API, which supports for passing chain of thought (CoT) between turns. Read below to migrate from your current model or API.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 13,
    "char_count": 228,
    "vector_index": 48,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_WNEyBjN25kxA5TKXGX0wNjFT",
    "text": "### Migrating from other models to GPT-5\n\nWe see improved intelligence because the Responses API can pass the previous turn's CoT to the model. This leads to fewer generated reasoning tokens, higher cache hit rates, and less latency. To learn more, see an [in-depth guide](https://cookbook.openai.com/examples/responses_api/reasoning_items) on the benefits of responses.\n\nWhen migrating to GPT-5 from an older OpenAI model, start by experimenting with reasoning levels and prompting strategies. Based on our testing, we recommend using our [prompt optimizer](http://platform.openai.com/chat/edit?optimize=true)—which automatically updates your prompts for GPT-5 based on our best practices—and following this model-specific guidance:\n\n*   **o3**: `gpt-5` with `medium` or `high` reasoning is a great replacement. Start with `medium` reasoning with prompt tuning, then increasing to `high` if you aren't getting the results you want.\n*   **gpt-4.1**: `gpt-5` with `minimal` or `low` reasoning is a strong alternative. Start with `minimal` and tune your prompts; increase to `low` if you need better performance.\n*   **o4-mini or gpt-4.1-mini**: `gpt-5-mini` with prompt tuning is a great replacement.\n*   **gpt-4.1-nano**: `gpt-5-nano` with prompt tuning is a great replacement.",
    "source_file": "Using GPT 5.md",
    "chunk_index": 14,
    "char_count": 1277,
    "vector_index": 49,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_AasxNRMvB0Tjs6lVRboTyypG",
    "text": "### GPT-5 parameter compatibility\n\n⚠️ **Important:** The following parameters are **not supported** when using GPT-5 models (e.g. `gpt-5`, `gpt-5-mini`, `gpt-5-nano`):\n\n*   `temperature`\n*   `top_p`\n*   `logprobs`\n\nRequests that include these fields will raise an error.\n\n**Instead, use the following GPT-5-specific controls:**\n\n*   **Reasoning depth:** `reasoning: { effort: \"minimal\" | \"low\" | \"medium\" | \"high\" }`\n*   **Output verbosity:** `text: { verbosity: \"low\" | \"medium\" | \"high\" }`\n*   **Output length:** `max_output_tokens`",
    "source_file": "Using GPT 5.md",
    "chunk_index": 15,
    "char_count": 534,
    "vector_index": 50,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_PWVFhLillgRoKRIqbMV005vR",
    "text": "### Migrating from Chat Completions to Responses API\n\nThe biggest difference, and main reason to migrate from Chat Completions to the Responses API for GPT-5, is support for passing chain of thought (CoT) between turns. See a full [comparison of the APIs](/docs/guides/responses-vs-chat-completions).\n\nPassing CoT exists only in the Responses API, and we've seen improved intelligence, fewer generated reasoning tokens, higher cache hit rates, and lower latency as a result of doing so. Most other parameters remain at parity, though the formatting is different. Here's how new parameters are handled differently between Chat Completions and the Responses API:\n\n**Reasoning effort**\n\nResponses API\n\nGenerate response with minimal reasoning\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/responses \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\",\n  \"reasoning\": {\n    \"effort\": \"minimal\"\n  }\n}'\n```\n\nChat Completions\n\nGenerate response with minimal reasoning\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/chat/completions \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How much gold would it take to coat the Statue of Liberty in a 1mm layer?\"\n    }\n  ],\n  \"reasoning_effort\": \"minimal\"\n}'\n```\n\n**Verbosity**\n\nResponses API\n\nControl verbosity\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/responses \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"What is the answer to the ultimate question of life, the universe, and everything?\",\n  \"text\": {\n    \"verbosity\": \"low\"\n  }\n}'\n```\n\nChat Completions\n\nControl verbosity\n\n```json\ncurl --request POST \\\n--url https://api.openai.com/v1/chat/completions \\\n--header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n--header 'Content-type: application/json' \\\n--data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"What is the answer to the ultimate question of life, the universe, and everything?\" }\n  ],\n  \"verbosity\": \"low\"\n}'\n```\n\n**Custom tools**\n\nResponses API\n\nCustom tool call\n\n```json\ncurl --request POST --url https://api.openai.com/v1/responses --header \"Authorization: Bearer $OPENAI_API_KEY\" --header 'Content-type: application/json' --data '{\n  \"model\": \"gpt-5\",\n  \"input\": \"Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry\",\n  \"tools\": [\n    {\n      \"type\": \"custom\",\n      \"name\": \"code_exec\",\n      \"description\": \"Executes arbitrary python code\"\n    }\n  ]\n}'\n```\n\nChat Completions\n\nCustom tool call\n\n```json\ncurl --request POST --url https://api.openai.com/v1/chat/completions --header \"Authorization: Bearer $OPENAI_API_KEY\" --header 'Content-type: application/json' --data '{\n  \"model\": \"gpt-5\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry\" }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"custom\",\n      \"custom\": {\n        \"name\": \"code_exec\",\n        \"description\": \"Executes arbitrary python code\"\n      }\n    }\n  ]\n}'\n```",
    "source_file": "Using GPT 5.md",
    "chunk_index": 16,
    "char_count": 3430,
    "vector_index": 51,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_k7EVdXzLxann8gR2iAUEEByU",
    "text": "Prompting guidance\n------------------\n\nWe specifically designed GPT-5 to excel at coding, frontend engineering, and tool-calling for agentic tasks. We also recommend iterating on prompts for GPT-5 using the [prompt optimizer](/chat/edit?optimize=true).\n\n[\n\nGPT-5 prompt optimizer\n\nCraft the perfect prompt for GPT-5 in the dashboard\n\n](/chat/edit?optimize=true)[\n\nGPT-5 prompting guide\n\nLearn full best practices for prompting GPT-5 models\n\n](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)[\n\nFrontend prompting for GPT-5\n\nSee prompt samples specific to frontend development\n\n](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)",
    "source_file": "Using GPT 5.md",
    "chunk_index": 17,
    "char_count": 654,
    "vector_index": 52,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_fWxLRSJpIvJN3CA1uXMaglyu",
    "text": "### GPT-5 is a reasoning model\n\nReasoning models like GPT-5 break problems down step by step, producing an internal chain of thought that encodes their reasoning. To maximize performance, pass these reasoning items back to the model: this avoids re-reasoning and keeps interactions closer to the model's training distribution. In multi-turn conversations, passing a `previous_response_id` automatically makes earlier reasoning items available. This is especially important when using tools—for example, when a function call requires an extra round trip. In these cases, either include them with `previous_response_id` or add them directly to `input`.\n\nLearn more about reasoning models and how to get the most out of them in our [reasoning guide](/docs/guides/reasoning).",
    "source_file": "Using GPT 5.md",
    "chunk_index": 18,
    "char_count": 771,
    "vector_index": 53,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_h9Uo9fVJQqjQRF98NH133NDx",
    "text": "Further reading\n---------------\n\n[GPT-5 prompting guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)\n\n[GPT-5 frontend guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)\n\n[GPT-5 new features guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools)\n\n[Cookbook on reasoning models](https://cookbook.openai.com/examples/responses_api/reasoning_items)\n\n[Comparison of Responses API vs. Chat Completions](/docs/guides/migrate-to-responses)",
    "source_file": "Using GPT 5.md",
    "chunk_index": 19,
    "char_count": 491,
    "vector_index": 54,
    "embedding_model": "text-embedding-3-large"
  },
  {
    "chunk_id": "call_Qz7KCuxxln4TtBwpLnAm4DO8",
    "text": "FAQ\n---\n\n1.  **How are these models integrated into ChatGPT?**\n    \n    In ChatGPT, there are two models: `gpt-5-chat` and `gpt-5-thinking`. They offer reasoning and minimal-reasoning capabilities, with a routing layer that selects the best model based on the user's question. Users can also invoke reasoning directly through the ChatGPT UI.\n    \n2.  **Will these models be supported in Codex?**\n    \n    Yes, `gpt-5` will be available in Codex and Codex CLI.\n    \n3.  **How does GPT-5 compare to GPT-5-Codex?**\n    \n    [`GPT-5-Codex`](/docs/models/gpt-5-codex) was specifically designed for use in Codex. Unlike `GPT-5`, which is a general-purpose model, we recommend using GPT-5-Codex only for agentic coding tasks in Codex or Codex-like environments, and GPT-5 for use cases in other domains. GPT-5-Codex is only available in the Responses API and supports low, medium, and high `reasoning_efforts` and function calling, structured outputs, and the `web_search` tool.\n    \n4.  **What is the deprecation plan for previous models?**\n    \n    Any model deprecations will be posted on our [deprecations page](/docs/deprecations#page-top). We'll send advanced notice of any model deprecations.\n    \n\nWas this page useful?",
    "source_file": "Using GPT 5.md",
    "chunk_index": 20,
    "char_count": 1220,
    "vector_index": 55,
    "embedding_model": "text-embedding-3-large"
  }
]
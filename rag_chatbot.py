"""
GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ - Part B: RAG å°è©±æ©Ÿå™¨äºº

æœ¬åœ°åŸ·è¡Œ:
    cd Week7
    uv run python rag_chatbot.py

Google Colab åŸ·è¡Œ:
    # 1. å®‰è£å¥—ä»¶
    !pip install openai faiss-cpu numpy gradio

    # 2. ä¸‹è¼‰å‘é‡è³‡æ–™åº«
    GDRIVE_LINK = "YOUR_GDRIVE_LINK_HERE"
    !gdown --fuzzy {GDRIVE_LINK}
    !unzip -o faiss_db.zip

    # 3. è¨­å®š API key
    from google.colab import userdata
    import os
    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

    # 4. åŸ·è¡Œç¨‹å¼
    !python rag_chatbot.py
"""

import os
import json
import numpy as np
import faiss
from openai import OpenAI
import gradio as gr
from typing import List, Dict, Tuple, Optional

# ===== å…¨åŸŸè¨­å®š =====
GDRIVE_FILE_ID = "YOUR_FILE_ID_HERE"  # å¾…å¡«å…¥
FAISS_DB_PATH = "faiss_db"
MAX_TOOL_ITERATIONS = 5

# System prompt for the chatbot
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ GPT-5 Response API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ã€‚

ä½ çš„å°ˆé•·é ˜åŸŸ:
- GPT-5 Response API çš„ä½¿ç”¨æ–¹å¼
- Function calling å’Œ custom tools
- Reasoning effort æ§åˆ¶ (minimal, low, medium, high)
- Text verbosity è¨­å®š
- Tool calling çš„æœ€ä½³å¯¦è¸

å›ç­”é¢¨æ ¼:
- å‹å–„ã€å°ˆæ¥­ã€æœ‰è€å¿ƒ
- æä¾›æ¸…æ¥šçš„ç¨‹å¼ç¢¼ç¯„ä¾‹
- è§£é‡‹æŠ€è¡“æ¦‚å¿µæ™‚ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼
- å¦‚æœä¸ç¢ºå®šç­”æ¡ˆ,èª å¯¦å‘ŠçŸ¥ä¸¦å»ºè­°æŸ¥é–±å®˜æ–¹æ–‡ä»¶

é‡è¦: ç•¶ä½ éœ€è¦æŸ¥è©¢ GPT-5 API ç›¸é—œè³‡æ–™æ™‚,è«‹ä½¿ç”¨ search_chunks å·¥å…·æœå°‹æ–‡ä»¶å…§å®¹ã€‚"""


# ===== 1. ç’°å¢ƒåµæ¸¬èˆ‡åˆå§‹åŒ– =====

def is_colab() -> bool:
    """åµæ¸¬æ˜¯å¦åœ¨ Google Colab ç’°å¢ƒ"""
    try:
        import google.colab
        return True
    except ImportError:
        return False


def setup_environment() -> str:
    """
    è¨­å®šç’°å¢ƒä¸¦å–å¾— API key

    Returns:
        str: OpenAI API key
    """
    if is_colab():
        from google.colab import userdata
        api_key = userdata.get('OPENAI_API_KEY')
        print("âœ“ Colab ç’°å¢ƒ: å¾ userdata è¼‰å…¥ API key")
    else:
        api_key = os.getenv('OPENAI_API_KEY')
        print("âœ“ æœ¬åœ°ç’°å¢ƒ: å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥ API key")

    if not api_key:
        raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY,è«‹ç¢ºèªç’°å¢ƒè¨­å®š")

    return api_key


def download_and_extract_faiss(gdrive_url: Optional[str] = None):
    """
    åœ¨ Colab ç’°å¢ƒä¸‹è¼‰ä¸¦è§£å£“ç¸® FAISS è³‡æ–™åº«
    æœ¬åœ°ç’°å¢ƒè·³éæ­¤æ­¥é©Ÿ

    Args:
        gdrive_url: Google Drive åˆ†äº«é€£çµ (å¯é¸)
    """
    if not is_colab():
        print("âœ“ æœ¬åœ°ç’°å¢ƒ: è·³éä¸‹è¼‰,ç›´æ¥ä½¿ç”¨æœ¬åœ° faiss_db")
        return

    if not gdrive_url or gdrive_url == "YOUR_GDRIVE_LINK_HERE":
        print("âš ï¸  è­¦å‘Š: å°šæœªè¨­å®š Google Drive é€£çµ")
        print("è«‹åœ¨ç¨‹å¼ç¢¼ä¸­è¨­å®š GDRIVE_FILE_ID æˆ–å‚³å…¥ gdrive_url åƒæ•¸")
        return

    print("â¬‡ï¸  Colab ç’°å¢ƒ: ä¸‹è¼‰ faiss_db.zip...")
    os.system(f'gdown --fuzzy "{gdrive_url}"')

    print("ğŸ“¦ è§£å£“ç¸® faiss_db.zip...")
    os.system('unzip -o faiss_db.zip')

    print("âœ“ FAISS è³‡æ–™åº«æº–å‚™å®Œæˆ")


# ===== 2. FAISS è¼‰å…¥ =====

def load_vectorstore() -> Tuple[faiss.Index, List[Dict], OpenAI]:
    """
    è¼‰å…¥ FAISS å‘é‡è³‡æ–™åº«

    Returns:
        Tuple[faiss.Index, List[Dict], OpenAI]: (FAISS index, metadata, OpenAI client)
    """
    print(f"ğŸ“‚ è¼‰å…¥å‘é‡è³‡æ–™åº«: {FAISS_DB_PATH}/")

    # è¼‰å…¥ FAISS index
    index_path = os.path.join(FAISS_DB_PATH, "index.faiss")
    if not os.path.exists(index_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° FAISS index: {index_path}")

    index = faiss.read_index(index_path)
    print(f"  âœ“ FAISS index è¼‰å…¥å®Œæˆ: {index.ntotal} å€‹å‘é‡")

    # è¼‰å…¥ metadata
    metadata_path = os.path.join(FAISS_DB_PATH, "metadata.json")
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° metadata: {metadata_path}")

    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    print(f"  âœ“ Metadata è¼‰å…¥å®Œæˆ: {len(metadata)} å€‹ chunks")

    # åˆå§‹åŒ– OpenAI client
    client = OpenAI()

    return index, metadata, client


# ===== 3. RAG æª¢ç´¢å‡½æ•¸ =====

def search_chunks(query: str, index: faiss.Index, metadata: List[Dict],
                 client: OpenAI, k: int = 5) -> List[Dict]:
    """
    ä½¿ç”¨ FAISS æœå°‹èˆ‡å•é¡Œæœ€ç›¸é—œçš„ chunks

    Args:
        query: ä½¿ç”¨è€…å•é¡Œ
        index: FAISS index
        metadata: Chunk metadata
        client: OpenAI client
        k: å›å‚³çµæœæ•¸é‡

    Returns:
        List[Dict]: ç›¸é—œçš„ chunks
    """
    # å°‡ query è½‰æˆ embedding
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=[query]
    )
    query_embedding = response.data[0].embedding

    # FAISS æœå°‹
    query_vector = np.array([query_embedding]).astype('float32')
    distances, indices = index.search(query_vector, k)

    # å–å¾—ç›¸é—œ chunks
    results = []
    for idx, distance in zip(indices[0], distances[0]):
        if idx < len(metadata):
            chunk = metadata[idx].copy()
            chunk['distance'] = float(distance)
            results.append(chunk)

    return results


def format_chunks_for_llm(chunks: List[Dict]) -> str:
    """
    æ ¼å¼åŒ– chunks æˆé©åˆ LLM é–±è®€çš„æ ¼å¼

    Args:
        chunks: æœå°‹çµæœ chunks

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„æ–‡å­—
    """
    formatted_parts = []

    for i, chunk in enumerate(chunks, 1):
        source = chunk.get('source_file', 'Unknown')
        text = chunk.get('text', '')

        formatted_parts.append(f"[æ–‡ä»¶ {i}: {source}]\n{text}")

    return "\n\n---\n\n".join(formatted_parts)


# ===== 4. GPT-5 å°è©±å¼•æ“ =====

def convert_gradio_history_to_openai(history: List[Tuple[str, str]]) -> List[Dict]:
    """
    è½‰æ› Gradio history æ ¼å¼ç‚º OpenAI messages æ ¼å¼

    Args:
        history: Gradio history [(user_msg, bot_msg), ...]

    Returns:
        List[Dict]: OpenAI messages æ ¼å¼
    """
    messages = []

    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})

    return messages


def chat_with_rag(user_message: str, gradio_history: List[Tuple[str, str]],
                 index: faiss.Index, metadata: List[Dict],
                 client: OpenAI) -> str:
    """
    ä¸»è¦å°è©±å‡½æ•¸,æ•´åˆ RAG æª¢ç´¢å’Œ GPT-5 å›ç­”

    Args:
        user_message: ä½¿ç”¨è€…è¼¸å…¥
        gradio_history: Gradio å°è©±æ­·å²
        index: FAISS index
        metadata: Chunk metadata
        client: OpenAI client

    Returns:
        str: AI å›ç­”
    """
    # 1. è½‰æ›å°è©±æ­·å²
    input_messages = convert_gradio_history_to_openai(gradio_history)
    input_messages.append({"role": "user", "content": user_message})

    # 2. å®šç¾© search_chunks tool
    tools = [{
        "type": "function",
        "name": "search_chunks",
        "description": "æœå°‹ GPT-5 Response API å®˜æ–¹æ–‡ä»¶å…§å®¹ã€‚ç•¶ä½¿ç”¨è€…è©¢å•é—œæ–¼ GPT-5 APIã€function callingã€tool callingã€reasoning effortã€verbosity ç­‰æŠ€è¡“å•é¡Œæ™‚,è«‹ä½¿ç”¨æ­¤å·¥å…·æœå°‹ç›¸é—œæ–‡ä»¶ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "æœå°‹é—œéµå­—æˆ–å•é¡Œ,ä¾‹å¦‚: 'custom tools', 'reasoning effort', 'function calling example'"
                }
            },
            "required": ["query"],
            "additionalProperties": False
        },
        "strict": True
    }]

    # 3. Tool calling loop
    iteration = 0

    while iteration < MAX_TOOL_ITERATIONS:
        print(f"\nğŸ”„ Tool calling iteration {iteration + 1}")

        # å‘¼å« GPT-5
        response = client.responses.create(
            model="gpt-5",
            reasoning={"effort": "low"},
            instructions=SYSTEM_PROMPT,
            input=input_messages,
            tools=tools
        )

        # å°‡ response.output çš„æ¯å€‹ item åŠ åˆ° input_messages
        # é‡è¦: ä¸è¦ç›´æ¥ += response.outputï¼Œè¦é€å€‹ append
        for item in response.output:
            input_messages.append(item)

        # æª¢æŸ¥æ˜¯å¦æœ‰ function calls ä¸¦åŸ·è¡Œ
        has_tool_calls = False

        for item in response.output:
            if item.type == "function_call":
                has_tool_calls = True
                function_name = item.name
                call_id = item.call_id

                print(f"  ğŸ“ GPT-5 å‘¼å«å·¥å…·: {function_name}")
                print(f"     åƒæ•¸: {item.arguments}")

                if function_name == "search_chunks":
                    # è§£æåƒæ•¸
                    args = json.loads(item.arguments)
                    query = args.get("query", "")

                    # åŸ·è¡Œæª¢ç´¢ (å›ºå®šä½¿ç”¨ k=5)
                    print(f"  ğŸ” åŸ·è¡Œæª¢ç´¢: query='{query}'")
                    chunks = search_chunks(query, index, metadata, client, k=5)

                    # æ ¼å¼åŒ–çµæœ
                    formatted_result = format_chunks_for_llm(chunks)

                    print(f"  âœ“ æ‰¾åˆ° {len(chunks)} å€‹ç›¸é—œæ–‡ä»¶ç‰‡æ®µ")

                    # åŠ å…¥ function_call_output
                    input_messages.append({
                        "type": "function_call_output",
                        "call_id": call_id,
                        "output": formatted_result
                    })

        # å¦‚æœæ²’æœ‰ tool calls,çµæŸå¾ªç’°
        if not has_tool_calls:
            print("  âœ“ GPT-5 å®Œæˆå›ç­”,ç„¡éœ€æ›´å¤šå·¥å…·")
            break

        iteration += 1

    if iteration >= MAX_TOOL_ITERATIONS:
        print(f"âš ï¸  è­¦å‘Š: é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({MAX_TOOL_ITERATIONS})")

    # 4. æå–æœ€çµ‚å›ç­”
    final_answer = response.output_text

    return final_answer


# ===== 5. Gradio ä»‹é¢ =====

def create_chatbot_interface(index: faiss.Index, metadata: List[Dict],
                            client: OpenAI) -> gr.Interface:
    """
    å»ºç«‹ Gradio chatbot ä»‹é¢

    Args:
        index: FAISS index
        metadata: Chunk metadata
        client: OpenAI client

    Returns:
        gr.Interface: Gradio ä»‹é¢
    """
    def chat_wrapper(message: str, history: List[Tuple[str, str]]) -> str:
        """Gradio ChatInterface çš„ wrapper"""
        return chat_with_rag(message, history, index, metadata, client)

    # å»ºç«‹ ChatInterface
    demo = gr.ChatInterface(
        fn=chat_wrapper,
        title="ğŸ¤– GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹",
        description="""
æˆ‘å¯ä»¥å”åŠ©ä½ å­¸ç¿’å’Œä½¿ç”¨ **GPT-5 Response API**!

**æˆ‘çš„å°ˆé•·é ˜åŸŸ:**
- Function calling å’Œ custom tools çš„ä½¿ç”¨
- Reasoning effort æ§åˆ¶ (minimal, low, medium, high)
- Text verbosity è¨­å®š
- Tool calling æœ€ä½³å¯¦è¸
- ç¨‹å¼ç¢¼ç¯„ä¾‹å’Œå¯¦ä½œå»ºè­°

**æç¤º:** æˆ‘æœƒè‡ªå‹•æœå°‹å®˜æ–¹æ–‡ä»¶ä¾†å›ç­”ä½ çš„å•é¡Œ,ä½ å¯ä»¥å•æˆ‘ä»»ä½•é—œæ–¼ GPT-5 API çš„æŠ€è¡“ç´°ç¯€!
        """.strip(),
        examples=[
            "å¦‚ä½•ä½¿ç”¨ custom tools?",
            "function calling çš„å®Œæ•´æµç¨‹æ˜¯ä»€éº¼?",
            "reasoning effort çš„ minimal å’Œ low æœ‰ä»€éº¼å·®åˆ¥?",
            "è«‹çµ¦æˆ‘ä¸€å€‹ tool calling loop çš„ç¨‹å¼ç¢¼ç¯„ä¾‹",
            "å¦‚ä½•è¨­å®š verbosity ä¾†æ§åˆ¶è¼¸å‡ºé•·åº¦?"
        ],
        theme=gr.themes.Soft()
    )

    return demo


# ===== 6. ä¸»ç¨‹å¼åŸ·è¡Œ =====

def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    print("=" * 60)
    print("ğŸš€ GPT-5 API ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ - Part B: RAG å°è©±æ©Ÿå™¨äºº")
    print("=" * 60)

    try:
        # 1. ç’°å¢ƒåµæ¸¬èˆ‡åˆå§‹åŒ–
        print("\n[1/4] ç’°å¢ƒè¨­å®š")
        api_key = setup_environment()

        # 2. ä¸‹è¼‰ FAISS (åƒ… Colab)
        print("\n[2/4] FAISS è³‡æ–™åº«æº–å‚™")
        download_and_extract_faiss()

        # 3. è¼‰å…¥å‘é‡è³‡æ–™åº«
        print("\n[3/4] è¼‰å…¥å‘é‡è³‡æ–™åº«")
        index, metadata, client = load_vectorstore()

        # 4. å•Ÿå‹• Gradio
        print("\n[4/4] å•Ÿå‹• Gradio ä»‹é¢")
        demo = create_chatbot_interface(index, metadata, client)

        print("\nâœ¨ ç³»çµ±æº–å‚™å®Œæˆ!")
        print("=" * 60)

        # å•Ÿå‹•ä»‹é¢
        demo.launch(
            share=True,  # ç”¢ç”Ÿ share link (Colab éœ€è¦)
            server_port=7860,
            show_error=True
        )

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise


if __name__ == "__main__":
    main()
